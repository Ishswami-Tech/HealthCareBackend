# Continuous Integration Workflow - Docker Only
name: CI/CD

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [develop, main]
    paths-ignore:
      - 'README.md'
      - '**/*.md'
      - '**/*.svg'

permissions:
  contents: read
  pull-requests: write
  checks: write
  packages: write
  security-events: write  # Required for CodeQL SARIF upload
  actions: read  # Required for CodeQL to access workflow run information

env:
  NODE_VERSION: '20'
  YARN_VERSION: '1.22.22'
  REGISTRY: ghcr.io
  IMAGE_REPO: ${{ github.repository }}
  IMAGE_NAME: healthcare-api
  # Note: IMAGE will be set in docker-build job with lowercase conversion

jobs:
  # Code Quality Checks
  # Skip linting and format checks for all branches - handled by git hooks
  # Pre-commit: Quick lint-staged (fast validation on staged files)
  # Pre-push: Full yarn build (comprehensive validation before pushing)
  # Docker build: Final validation with yarn build (linting, formatting, optimizations)
  lint:
    name: Lint & Format Check (Skipped)
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4

      - name: Skip linting and format checks
        run: |
          echo "â­ï¸  Skipping linting and format checks in CI"
          echo "âœ… Code is validated by pre-commit (lint-staged) and pre-push (yarn build) hooks"
          echo "âœ… Pre-push ensures only buildable code is pushed (full validation)"
          echo "âœ… Docker build runs final validation with yarn build (linting, formatting, optimizations)"
          echo "âœ… Prisma client is generated during Docker build"
          exit 0

  # Security Scanning
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v4
        continue-on-error: true  # Don't fail if Advanced Security is not enabled
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Dependency Check
        run: |
          npx audit-ci --moderate

  # TypeScript Build
  # Docker Build & Push
  # Note: Build happens inside Docker, no separate build job needed
  # Prisma client generation happens during Docker build (see Dockerfile line 34)
  # Linting and formatting are done in git hooks (pre-commit + pre-push) and Docker build
  # Pre-push hook runs full yarn build, ensuring only buildable code reaches CI
  docker-build:
    name: Docker Build & Push
    runs-on: ubuntu-latest
    # Lint job is skipped (handled by pre-push hook), only depend on security
    needs: [lint, security]
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Set image name (lowercase)
        id: image
        run: |
          REPO_LOWER=$(echo "${{ env.IMAGE_REPO }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_FULL="ghcr.io/${REPO_LOWER}/${{ env.IMAGE_NAME }}"
          echo "image=${IMAGE_FULL}" >> $GITHUB_OUTPUT
          echo "Image name: ${IMAGE_FULL}"

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.image.outputs.image }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: devops/docker/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Output image digest
        run: |
          echo "Image pushed: ${{ steps.meta.outputs.tags }}"

  # Change Detection
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    outputs:
      infra-changed: ${{ steps.filter.outputs.infra }}
      app-changed: ${{ steps.filter.outputs.app }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Detect infrastructure and application changes
        uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            infra:
              - 'devops/docker/Dockerfile.postgres'
              - 'devops/docker/Dockerfile.dragonfly'
              - 'devops/docker/Dockerfile.openvidu'
              - 'devops/docker/docker-compose.prod.yml'
              - 'prisma/migrations/**'
            app:
              - 'src/**'
              - 'package.json'
              - 'yarn.lock'
              - 'tsconfig.json'
              - 'devops/docker/Dockerfile'

      - name: Output changes
        run: |
          echo "Infrastructure changed: ${{ steps.filter.outputs.infra }}"
          echo "Application changed: ${{ steps.filter.outputs.app }}"

  # Infrastructure Health Check
  check-infrastructure:
    name: Check Infrastructure Health
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment: production
    needs: [detect-changes, docker-build]
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy health check script
        run: |
          # Copy to temp files first, then move to final location (prevents "Text file busy" error)
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/health-check.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-check.sh.tmp
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh.tmp
          
          # Move temp files to final location atomically
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            mv /tmp/health-check.sh.tmp /tmp/health-check.sh
            mv /tmp/utils.sh.tmp /tmp/utils.sh
            chmod +x /tmp/health-check.sh /tmp/utils.sh
          ENDSSH

      - name: Run health check
        id: health-check
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            chmod +x /tmp/health-check.sh /tmp/utils.sh
            export SCRIPT_DIR="/tmp"
            export AUTO_RECREATE_MISSING="true"
            source /tmp/utils.sh
            
            # Ensure files are fully written before execution (fix "Text file busy" error)
            sync
            sleep 1
            
            /tmp/health-check.sh > /tmp/health-status.json 2>&1
            echo "EXIT_CODE=$?" > /tmp/health-exit.txt
          ENDSSH
          
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-status.json /tmp/health-status.json || true
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-exit.txt /tmp/health-exit.txt || true
          
          if [[ -f /tmp/health-exit.txt ]]; then
            EXIT_CODE=$(cat /tmp/health-exit.txt | grep EXIT_CODE | cut -d= -f2)
            if [[ "$EXIT_CODE" == "0" ]]; then
              echo "infra-healthy=true" >> $GITHUB_OUTPUT
              echo "infra-status=healthy" >> $GITHUB_OUTPUT
            else
              echo "infra-healthy=false" >> $GITHUB_OUTPUT
              if [[ "$EXIT_CODE" == "3" ]]; then
                echo "infra-status=missing" >> $GITHUB_OUTPUT
              else
                echo "infra-status=unhealthy" >> $GITHUB_OUTPUT
              fi
            fi
          else
            echo "infra-healthy=false" >> $GITHUB_OUTPUT
            echo "infra-status=unknown" >> $GITHUB_OUTPUT
          fi

      - name: Display health status
        run: |
          echo "Infrastructure Health: ${{ steps.health-check.outputs.infra-healthy }}"
          echo "Infrastructure Status: ${{ steps.health-check.outputs.infra-status }}"
          if [[ -f /tmp/health-status.json ]]; then
            cat /tmp/health-status.json
          fi

  # Backup Infrastructure
  backup-infrastructure:
    name: Backup Infrastructure
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main' && 
      (needs.detect-changes.outputs.infra-changed == 'true' || 
       needs.check-infrastructure.outputs.infra-healthy == 'false')
    environment: production
    needs: [detect-changes, check-infrastructure]
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy backup scripts
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/backup.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/backup.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh

      - name: Run backup
        id: backup
        env:
          S3_ENABLED: ${{ vars.S3_ENABLED }}
          S3_PROVIDER: ${{ vars.S3_PROVIDER }}
          S3_ENDPOINT: ${{ vars.S3_ENDPOINT }}
          S3_REGION: ${{ vars.S3_REGION }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
          S3_FORCE_PATH_STYLE: ${{ vars.S3_FORCE_PATH_STYLE }}
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            set -e
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            export S3_ENABLED="${{ vars.S3_ENABLED }}"
            export S3_PROVIDER="${{ vars.S3_PROVIDER }}"
            export S3_ENDPOINT="${{ vars.S3_ENDPOINT }}"
            export S3_REGION="${{ vars.S3_REGION }}"
            export S3_BUCKET="${{ vars.S3_BUCKET }}"
            export S3_ACCESS_KEY_ID="${{ secrets.S3_ACCESS_KEY_ID }}"
            export S3_SECRET_ACCESS_KEY="${{ secrets.S3_SECRET_ACCESS_KEY }}"
            export S3_FORCE_PATH_STYLE="${{ vars.S3_FORCE_PATH_STYLE }}"
            
            # Install rclone (Contabo's recommended tool) or s3cmd as fallback
            # Contabo recommends rclone: https://help.contabo.com/en/support/solutions/articles/103000305592-what-is-the-rclone-tool-
            if ! command -v rclone &> /dev/null && ! command -v s3cmd &> /dev/null; then
              echo "ðŸ“¦ Installing rclone for Contabo S3 (Contabo's recommended tool)..."
              # Install rclone using official installer (recommended by Contabo)
              if curl -fsSL https://rclone.org/install.sh | sudo bash; then
                echo "âœ… rclone installed successfully"
              else
                echo "âš ï¸  rclone installation failed, trying s3cmd fallback..."
                if command -v apt-get &> /dev/null; then
                  sudo apt-get update -qq
                  sudo apt-get install -y s3cmd || {
                    echo "âš ï¸  apt-get install failed, trying pip3..."
                    if command -v pip3 &> /dev/null; then
                      pip3 install --user s3cmd || {
                        echo "âŒ Failed to install both rclone and s3cmd"
                        exit 1
                      }
                      export PATH="$HOME/.local/bin:$PATH"
                    else
                      echo "âŒ Cannot install rclone or s3cmd automatically"
                      exit 1
                    }
                  }
                elif command -v pip3 &> /dev/null; then
                  pip3 install --user s3cmd || {
                    echo "âŒ Failed to install both rclone and s3cmd"
                    exit 1
                  }
                  export PATH="$HOME/.local/bin:$PATH"
                else
                  echo "âŒ Cannot install rclone or s3cmd automatically"
                  exit 1
                fi
                echo "âœ… s3cmd installed as fallback"
              fi
            elif command -v rclone &> /dev/null; then
              echo "âœ… rclone is already installed"
            elif command -v s3cmd &> /dev/null; then
              echo "âœ… s3cmd is already installed (rclone preferred but s3cmd will work)"
            fi
            
            cd /opt/healthcare-backend/devops/docker || exit 1
            
            # CRITICAL: Ensure containers are running before backup
            echo "ðŸ” Ensuring PostgreSQL and Dragonfly containers are running for backup..."
            
            # Check and start PostgreSQL if needed
            if ! docker ps --format '{{.Names}}' | grep -q "^postgres$"; then
              echo "âš ï¸  PostgreSQL container not running - starting for backup..."
              docker compose -f docker-compose.prod.yml --profile infrastructure up -d postgres || {
                echo "âŒ Failed to start PostgreSQL for backup"
                exit 1
              }
              echo "â³ Waiting for PostgreSQL to be ready..."
              MAX_RETRIES=30
              RETRY_COUNT=0
              while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                if docker exec postgres pg_isready -U postgres -d userdb >/dev/null 2>&1; then
                  echo "âœ… PostgreSQL is ready for backup"
                  break
                fi
                RETRY_COUNT=$((RETRY_COUNT + 1))
                sleep 2
              done
              if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
                echo "âŒ PostgreSQL did not become ready for backup"
                exit 1
              fi
            else
              echo "âœ… PostgreSQL container is running"
            fi
            
            # Check and start Dragonfly if needed
            if ! docker ps --format '{{.Names}}' | grep -q "^dragonfly$"; then
              echo "âš ï¸  Dragonfly container not running - starting for backup..."
              docker compose -f docker-compose.prod.yml --profile infrastructure up -d dragonfly || {
                echo "âŒ Failed to start Dragonfly for backup"
                exit 1
              }
              echo "â³ Waiting for Dragonfly to be ready..."
              MAX_RETRIES=20
              RETRY_COUNT=0
              while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                if docker exec dragonfly redis-cli -p 6379 ping >/dev/null 2>&1; then
                  echo "âœ… Dragonfly is ready for backup"
                  break
                fi
                RETRY_COUNT=$((RETRY_COUNT + 1))
                sleep 2
              done
              if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
                echo "âš ï¸  Dragonfly did not become ready (backup may fail for Dragonfly)"
              fi
            else
              echo "âœ… Dragonfly container is running"
            fi
            
            # CRITICAL: Create backup of PostgreSQL and Dragonfly data
            echo "ðŸ’¾ Creating backup of PostgreSQL and Dragonfly data..."
            echo "âš ï¸  This backup is CRITICAL - data will be lost if backup fails!"
            chmod +x /tmp/backup.sh
            BACKUP_ID=\$(/tmp/backup.sh pre-deployment) || {
              echo "âŒ Backup failed - ABORTING to prevent data loss"
              exit 1
            }
            echo "âœ… Backup created successfully (ID: \${BACKUP_ID})"
            echo "BACKUP_ID=\${BACKUP_ID}" > /tmp/backup-id.txt
          ENDSSH
          
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/backup-id.txt /tmp/backup-id.txt
          BACKUP_ID=\$(grep BACKUP_ID /tmp/backup-id.txt | cut -d= -f2)
          echo "backup-id=\${BACKUP_ID}" >> $GITHUB_OUTPUT
          echo "Backup ID: \${BACKUP_ID}"

  # Debug Infrastructure
  debug-infrastructure:
    name: Debug Infrastructure
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main' && 
      needs.check-infrastructure.outputs.infra-healthy == 'false' &&
      needs.detect-changes.outputs.infra-changed != 'true'
    environment: production
    needs: [detect-changes, check-infrastructure]
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy diagnostic scripts
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/diagnose.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/diagnose.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh

      - name: Run diagnostics and auto-fix
        id: debug
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            chmod +x /tmp/diagnose.sh
            if /tmp/diagnose.sh > /tmp/debug-result.json 2>&1; then
              echo "debug-status=fixed" > /tmp/debug-status.txt
            else
              echo "debug-status=failed" > /tmp/debug-status.txt
            fi
          ENDSSH
          
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/debug-status.txt /tmp/debug-status.txt
          DEBUG_STATUS=\$(grep debug-status /tmp/debug-status.txt | cut -d= -f2)
          echo "debug-status=\${DEBUG_STATUS}" >> $GITHUB_OUTPUT

  # Recreate Infrastructure
  recreate-infrastructure:
    name: Recreate Infrastructure
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main' && 
      (needs.detect-changes.outputs.infra-changed == 'true' || 
       (needs.check-infrastructure.outputs.infra-healthy == 'false' && 
        (needs.debug-infrastructure.result == 'failure' || needs.debug-infrastructure.result == 'skipped')))
    environment: production
    needs: [detect-changes, check-infrastructure, backup-infrastructure, debug-infrastructure]
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Recreate infrastructure containers
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            set -e
            cd /opt/healthcare-backend/devops/docker || exit 1
            
            # CRITICAL: Verify data volumes are preserved before recreation
            echo "ðŸ” Verifying data volumes are preserved..."
            VOLUMES=("postgres_data" "dragonfly_data")
            VOLUME_PATHS=("/opt/healthcare-backend/data/postgres" "/opt/healthcare-backend/data/dragonfly")
            
            for i in "${!VOLUMES[@]}"; do
              VOLUME="${VOLUMES[$i]}"
              VOLUME_PATH="${VOLUME_PATHS[$i]}"
              
              # Check if volume exists
              if docker volume inspect "docker_${VOLUME}" >/dev/null 2>&1; then
                echo "âœ… Volume ${VOLUME} exists"
                
                # For bind mounts, verify the host path exists
                if [[ -d "$VOLUME_PATH" ]]; then
                  echo "âœ… Volume path verified: ${VOLUME_PATH}"
                else
                  echo "âš ï¸  Volume path missing: ${VOLUME_PATH} (creating...)"
                  mkdir -p "$VOLUME_PATH" || {
                    echo "âŒ Failed to create volume path: ${VOLUME_PATH}"
                    exit 1
                  }
                fi
              else
                echo "âš ï¸  Volume ${VOLUME} does not exist (will be created)"
              fi
            done
            
            # CRITICAL: Ensure containers are running before graceful stop (for data flush)
            echo "ðŸ” Ensuring containers are running for graceful shutdown..."
            CONTAINERS=("postgres" "dragonfly")
            
            for CONTAINER in "${CONTAINERS[@]}"; do
              if ! docker ps --format '{{.Names}}' | grep -q "^${CONTAINER}$"; then
                echo "âš ï¸  Container ${CONTAINER} not running - starting for graceful shutdown..."
                docker compose -f docker-compose.prod.yml --profile infrastructure up -d "${CONTAINER}" || {
                  echo "âŒ Failed to start ${CONTAINER}"
                  exit 1
                }
                # Wait for container to be ready
                echo "â³ Waiting for ${CONTAINER} to be ready..."
                sleep 5
              fi
            done
            
            # CRITICAL: Stop existing infrastructure containers gracefully
            echo "ðŸ›‘ Stopping infrastructure containers gracefully (ensuring data flush)..."
            docker compose -f docker-compose.prod.yml --profile infrastructure stop || {
              echo "âš ï¸  Graceful stop had issues, forcing stop..."
              docker compose -f docker-compose.prod.yml --profile infrastructure kill || true
            }
            
            # Wait for data to flush to disk
            echo "â³ Waiting for data to flush to disk..."
            sleep 3
            
            # CRITICAL: Recreate infrastructure containers (volumes are preserved)
            echo "ðŸ”„ Recreating infrastructure containers (volumes will be preserved)..."
            docker compose -f docker-compose.prod.yml --profile infrastructure up -d --force-recreate || {
              echo "âŒ Infrastructure recreation failed"
              exit 1
            }
            
            # Wait for containers to be healthy
            echo "â³ Waiting for infrastructure containers to be healthy..."
            sleep 15
            
            # Wait for PostgreSQL to be ready
            echo "â³ Waiting for PostgreSQL to be ready..."
            MAX_RETRIES=30
            RETRY_COUNT=0
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              if docker exec postgres pg_isready -U postgres -d userdb >/dev/null 2>&1; then
                echo "âœ… PostgreSQL is ready"
                break
              fi
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "â³ Waiting for PostgreSQL... (${RETRY_COUNT}/${MAX_RETRIES})"
              sleep 2
            done
            
            if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
              echo "âŒ PostgreSQL did not become ready"
              exit 1
            fi
            
            # Wait for Dragonfly to be ready
            echo "â³ Waiting for Dragonfly to be ready..."
            MAX_RETRIES=20
            RETRY_COUNT=0
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              if docker exec dragonfly redis-cli -p 6379 ping >/dev/null 2>&1; then
                echo "âœ… Dragonfly is ready"
                break
              fi
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "â³ Waiting for Dragonfly... (${RETRY_COUNT}/${MAX_RETRIES})"
              sleep 2
            done
            
            if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
              echo "âŒ Dragonfly did not become ready"
              exit 1
            fi
            
            # Check container status
            echo "ðŸ“Š Container status:"
            docker compose -f docker-compose.prod.yml --profile infrastructure ps
            
            # Verify volumes still exist
            echo "ðŸ” Verifying volumes after recreation..."
            for i in "${!VOLUMES[@]}"; do
              VOLUME="${VOLUMES[$i]}"
              VOLUME_PATH="${VOLUME_PATHS[$i]}"
              
              if docker volume inspect "docker_${VOLUME}" >/dev/null 2>&1; then
                echo "âœ… Volume ${VOLUME} preserved"
              else
                echo "âš ï¸  Volume ${VOLUME} not found (may be created on first use)"
              fi
              
              if [[ -d "$VOLUME_PATH" ]]; then
                echo "âœ… Volume path preserved: ${VOLUME_PATH}"
              else
                echo "âš ï¸  Volume path missing: ${VOLUME_PATH}"
              fi
            done
            
            echo "âœ… Infrastructure recreation completed successfully"
          ENDSSH

  # Restore Backup
  restore-backup:
    name: Restore Backup
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main' && 
      needs.backup-infrastructure.result == 'success' &&
      needs.backup-infrastructure.outputs.backup-id != ''
    environment: production
    needs: [backup-infrastructure, recreate-infrastructure]
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy restore scripts
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/restore.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/restore.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh

      - name: Restore from backup
        env:
          S3_ENABLED: ${{ vars.S3_ENABLED }}
          S3_PROVIDER: ${{ vars.S3_PROVIDER }}
          S3_ENDPOINT: ${{ vars.S3_ENDPOINT }}
          S3_REGION: ${{ vars.S3_REGION }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
          S3_FORCE_PATH_STYLE: ${{ vars.S3_FORCE_PATH_STYLE }}
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            export S3_ENABLED="${{ vars.S3_ENABLED }}"
            export S3_PROVIDER="${{ vars.S3_PROVIDER }}"
            export S3_ENDPOINT="${{ vars.S3_ENDPOINT }}"
            export S3_REGION="${{ vars.S3_REGION }}"
            export S3_BUCKET="${{ vars.S3_BUCKET }}"
            export S3_ACCESS_KEY_ID="${{ secrets.S3_ACCESS_KEY_ID }}"
            export S3_SECRET_ACCESS_KEY="${{ secrets.S3_SECRET_ACCESS_KEY }}"
            export S3_FORCE_PATH_STYLE="${{ vars.S3_FORCE_PATH_STYLE }}"
            
            # Install rclone (Contabo's recommended tool) or s3cmd as fallback
            if ! command -v rclone &> /dev/null && ! command -v s3cmd &> /dev/null; then
              echo "ðŸ“¦ Installing rclone for Contabo S3 (Contabo's recommended tool)..."
              if curl -fsSL https://rclone.org/install.sh | sudo bash; then
                echo "âœ… rclone installed successfully"
              else
                echo "âš ï¸  rclone installation failed, trying s3cmd fallback..."
                if command -v apt-get &> /dev/null; then
                  sudo apt-get update -qq
                  sudo apt-get install -y s3cmd || {
                    if command -v pip3 &> /dev/null; then
                      pip3 install --user s3cmd && export PATH="$HOME/.local/bin:$PATH" || {
                        echo "âŒ Failed to install both rclone and s3cmd"
                        exit 1
                      }
                    else
                      echo "âŒ Cannot install rclone or s3cmd automatically"
                      exit 1
                    }
                  }
                elif command -v pip3 &> /dev/null; then
                  pip3 install --user s3cmd && export PATH="$HOME/.local/bin:$PATH" || {
                    echo "âŒ Failed to install both rclone and s3cmd"
                    exit 1
                  }
                else
                  echo "âŒ Cannot install rclone or s3cmd automatically"
                  exit 1
                fi
                echo "âœ… s3cmd installed as fallback"
              fi
            elif command -v rclone &> /dev/null; then
              echo "âœ… rclone is already installed"
            elif command -v s3cmd &> /dev/null; then
              echo "âœ… s3cmd is already installed (rclone preferred but s3cmd will work)"
            fi
            
            chmod +x /tmp/restore.sh
            # Validate backup-id to prevent command injection
            BACKUP_ID="${{ needs.backup-infrastructure.outputs.backup-id }}"
            if [[ ! "$BACKUP_ID" =~ ^[a-zA-Z0-9_-]+$ ]]; then
              echo "ERROR: Invalid backup ID format"
              exit 1
            fi
            /tmp/restore.sh "$BACKUP_ID"
          ENDSSH

  # Verify Infrastructure
  # Runs after deployment to verify infrastructure is healthy post-deployment
  verify-infrastructure:
    name: Verify Infrastructure
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main' &&
      (
        (needs.recreate-infrastructure.result == 'success' && 
         (needs.restore-backup.result == 'success' || needs.restore-backup.result == 'skipped')) ||
        (needs.check-infrastructure.outputs.infra-healthy == 'true' && 
         needs.backup-infrastructure.result == 'success')
      )
    environment: production
    needs: [check-infrastructure, backup-infrastructure, recreate-infrastructure, restore-backup]
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy verify script
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/verify.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/verify.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh

      - name: Verify infrastructure
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            chmod +x /tmp/verify.sh
            # Run full deployment verification (infrastructure + application)
            /tmp/verify.sh deployment
          ENDSSH

  # Deploy to Production Server (Main Branch Only)
  # Runs when:
  # 1. Docker build succeeds AND infrastructure is healthy (normal flow)
  # 2. Infrastructure was recreated successfully (recovery flow) - triggers app deployment
  deploy:
    name: Deploy to Production
    if: |
      always() &&
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main' &&
      needs.docker-build.result == 'success' &&
      (
        needs.verify-infrastructure.result == 'success' ||
        (needs.check-infrastructure.outputs.infra-healthy == 'true' && 
         needs.backup-infrastructure.result == 'success')
      )
    runs-on: ubuntu-latest
    environment: production  # Use production environment secrets
    needs: [docker-build, check-infrastructure, backup-infrastructure, verify-infrastructure, ensure-infrastructure-health, recreate-infrastructure, restore-backup]
    concurrency:
      group: deploy-production
      cancel-in-progress: false
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Create .env.production file
        run: |
          cat > /tmp/.env.production << EOF
          NODE_ENV=${{ vars.NODE_ENV }}
          IS_DEV=${{ vars.IS_DEV }}
          DATABASE_URL=${{ secrets.DATABASE_URL }}
          DIRECT_URL=${{ secrets.DIRECT_URL }}
          DATABASE_SQL_INJECTION_PREVENTION_ENABLED=${{ vars.DATABASE_SQL_INJECTION_PREVENTION_ENABLED }}
          DATABASE_ROW_LEVEL_SECURITY_ENABLED=${{ vars.DATABASE_ROW_LEVEL_SECURITY_ENABLED }}
          DATABASE_DATA_MASKING_ENABLED=${{ vars.DATABASE_DATA_MASKING_ENABLED }}
          DATABASE_RATE_LIMITING_ENABLED=${{ vars.DATABASE_RATE_LIMITING_ENABLED }}
          DATABASE_READ_REPLICAS_ENABLED=${{ vars.DATABASE_READ_REPLICAS_ENABLED }}
          DATABASE_READ_REPLICAS_STRATEGY=${{ vars.DATABASE_READ_REPLICAS_STRATEGY }}
          DATABASE_READ_REPLICAS_URLS=${{ vars.DATABASE_READ_REPLICAS_URLS }}
          CACHE_ENABLED=${{ vars.CACHE_ENABLED }}
          CACHE_PROVIDER=${{ vars.CACHE_PROVIDER }}
          DRAGONFLY_ENABLED=${{ vars.DRAGONFLY_ENABLED }}
          DRAGONFLY_HOST=${{ vars.DRAGONFLY_HOST }}
          DRAGONFLY_PORT=${{ vars.DRAGONFLY_PORT }}
          DRAGONFLY_KEY_PREFIX=${{ vars.DRAGONFLY_KEY_PREFIX }}
          DRAGONFLY_PASSWORD=${{ secrets.DRAGONFLY_PASSWORD }}
          REDIS_HOST=${{ vars.REDIS_HOST }}
          REDIS_PORT=${{ vars.REDIS_PORT }}
          REDIS_TTL=${{ vars.REDIS_TTL }}
          REDIS_PREFIX=${{ vars.REDIS_PREFIX }}
          REDIS_ENABLED=${{ vars.REDIS_ENABLED }}
          REDIS_PASSWORD=${{ vars.REDIS_PASSWORD }}
          PORT=${{ vars.PORT }}
          API_PREFIX=${{ vars.API_PREFIX }}
          HOST=${{ vars.HOST }}
          BIND_ADDRESS=${{ vars.BIND_ADDRESS }}
          BASE_URL=${{ vars.BASE_URL }}
          API_URL=${{ vars.API_URL }}
          FRONTEND_URL=${{ vars.FRONTEND_URL }}
          MAIN_DOMAIN=${{ vars.MAIN_DOMAIN }}
          API_DOMAIN=${{ vars.API_DOMAIN }}
          FRONTEND_DOMAIN=${{ vars.FRONTEND_DOMAIN }}
          JWT_SECRET=${{ secrets.JWT_SECRET }}
          JWT_EXPIRATION=${{ vars.JWT_EXPIRATION }}
          JWT_ACCESS_EXPIRES_IN=${{ vars.JWT_ACCESS_EXPIRES_IN }}
          JWT_REFRESH_EXPIRES_IN=${{ vars.JWT_REFRESH_EXPIRES_IN }}
          JWT_REFRESH_SECRET=${{ secrets.JWT_REFRESH_SECRET }}
          PRISMA_SCHEMA_PATH=${{ vars.PRISMA_SCHEMA_PATH }}
          LOG_LEVEL=${{ vars.LOG_LEVEL }}
          ENABLE_AUDIT_LOGS=${{ vars.ENABLE_AUDIT_LOGS }}
          RATE_LIMIT_ENABLED=${{ vars.RATE_LIMIT_ENABLED }}
          RATE_LIMIT_TTL=${{ vars.RATE_LIMIT_TTL }}
          RATE_LIMIT_MAX=${{ vars.RATE_LIMIT_MAX }}
          API_RATE_LIMIT=${{ vars.API_RATE_LIMIT }}
          AUTH_RATE_LIMIT=${{ vars.AUTH_RATE_LIMIT }}
          HEAVY_RATE_LIMIT=${{ vars.HEAVY_RATE_LIMIT }}
          USER_RATE_LIMIT=${{ vars.USER_RATE_LIMIT }}
          HEALTH_RATE_LIMIT=${{ vars.HEALTH_RATE_LIMIT }}
          MAX_AUTH_ATTEMPTS=${{ vars.MAX_AUTH_ATTEMPTS }}
          AUTH_ATTEMPT_WINDOW=${{ vars.AUTH_ATTEMPT_WINDOW }}
          MAX_CONCURRENT_SESSIONS=${{ vars.MAX_CONCURRENT_SESSIONS }}
          SESSION_INACTIVITY_THRESHOLD=${{ vars.SESSION_INACTIVITY_THRESHOLD }}
          SECURITY_RATE_LIMIT=${{ vars.SECURITY_RATE_LIMIT }}
          SECURITY_RATE_LIMIT_MAX=${{ vars.SECURITY_RATE_LIMIT_MAX }}
          SECURITY_RATE_LIMIT_WINDOW_MS=${{ vars.SECURITY_RATE_LIMIT_WINDOW_MS }}
          TRUST_PROXY=${{ vars.TRUST_PROXY }}
          EMAIL_PROVIDER=${{ vars.EMAIL_PROVIDER }}
          ZEPTOMAIL_ENABLED=${{ vars.ZEPTOMAIL_ENABLED }}
          ZEPTOMAIL_SEND_MAIL_TOKEN=${{ secrets.ZEPTOMAIL_SEND_MAIL_TOKEN }}
          ZEPTOMAIL_FROM_EMAIL=${{ vars.ZEPTOMAIL_FROM_EMAIL }}
          ZEPTOMAIL_FROM_NAME=${{ vars.ZEPTOMAIL_FROM_NAME }}
          ZEPTOMAIL_BOUNCE_ADDRESS=${{ vars.ZEPTOMAIL_BOUNCE_ADDRESS }}
          ZEPTOMAIL_API_BASE_URL=${{ vars.ZEPTOMAIL_API_BASE_URL }}
          
          # Clinic-Specific Email Configuration (Multi-Tenant)
          # ===================================================
          # ARCHITECTURE: Single Backend API, Multiple Frontends
          # - ONE backend API (API_URL) serves ALL clinics
          # - MULTIPLE frontends (one per clinic) connect to the SAME API
          # - Each frontend sends X-Clinic-ID header to identify which clinic it represents
          # - Only clinic-related credentials differ (email, WhatsApp, SMS, frontend URLs, etc.)
          # - All clinics share the same backend infrastructure (database, cache, services)
          #
          # Pattern: CLINIC_{SANITIZED_CLINIC_NAME}_{CONFIG_KEY}
          # Clinic names are sanitized: spaces/special chars â†’ underscores, uppercase
          # Example: "Vishwamurti Ayurvedelay" â†’ "VISHWAMURTI_AYURVEDELAY"
          #
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example for "Aadesh Ayurvedalay":
          # CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_SEND_MAIL_TOKEN=${{ vars.CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_SEND_MAIL_TOKEN }}
          # CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_FROM_EMAIL=${{ vars.CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_FROM_EMAIL }}
          # CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_FROM_NAME=${{ vars.CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_FROM_NAME }}
          # CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_BOUNCE_ADDRESS=${{ vars.CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_BOUNCE_ADDRESS }}
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_SEND_MAIL_TOKEN=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_SEND_MAIL_TOKEN }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_FROM_EMAIL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_FROM_EMAIL }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_FROM_NAME=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_FROM_NAME }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_BOUNCE_ADDRESS=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_BOUNCE_ADDRESS }}
          
          # CORS Configuration - Single API, Multiple Frontends
          # =====================================================
          # IMPORTANT: Include ALL clinic frontend URLs in CORS_ORIGIN
          # Format: Comma-separated list (no spaces after commas)
          # Example: "https://ishswami.in,https://www.ishswami.in,https://vishwamurti.viddhakarma.com"
          # All frontends connect to the SAME backend API (API_URL)
          CORS_ORIGIN=${{ vars.CORS_ORIGIN }}
          CORS_CREDENTIALS=${{ vars.CORS_CREDENTIALS }}
          CORS_METHODS=${{ vars.CORS_METHODS }}
          SWAGGER_URL=${{ vars.SWAGGER_URL }}
          BULL_BOARD_URL=${{ vars.BULL_BOARD_URL }}
          SOCKET_URL=${{ secrets.SOCKET_URL }}
          PRISMA_STUDIO_URL=${{ vars.PRISMA_STUDIO_URL }}
          PGADMIN_URL=${{ vars.PGADMIN_URL }}
          WHATSAPP_ENABLED=${{ vars.WHATSAPP_ENABLED }}
          WHATSAPP_API_URL=${{ vars.WHATSAPP_API_URL }}
          WHATSAPP_API_KEY=${{ vars.WHATSAPP_API_KEY }}
          WHATSAPP_PHONE_NUMBER_ID=${{ vars.WHATSAPP_PHONE_NUMBER_ID }}
          WHATSAPP_BUSINESS_ACCOUNT_ID=${{ vars.WHATSAPP_BUSINESS_ACCOUNT_ID }}
          WHATSAPP_OTP_TEMPLATE_ID=${{ vars.WHATSAPP_OTP_TEMPLATE_ID }}
          WHATSAPP_APPOINTMENT_TEMPLATE_ID=${{ vars.WHATSAPP_APPOINTMENT_TEMPLATE_ID }}
          WHATSAPP_PRESCRIPTION_TEMPLATE_ID=${{ vars.WHATSAPP_PRESCRIPTION_TEMPLATE_ID }}
          
          # Clinic-Specific WhatsApp Configuration (Multi-Tenant)
          # =======================================================
          # Each clinic can have separate WhatsApp credentials
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_API_KEY=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_API_KEY }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_PHONE_NUMBER_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_PHONE_NUMBER_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_BUSINESS_ACCOUNT_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_BUSINESS_ACCOUNT_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_OTP_TEMPLATE_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_OTP_TEMPLATE_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_APPOINTMENT_TEMPLATE_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_APPOINTMENT_TEMPLATE_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_PRESCRIPTION_TEMPLATE_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_PRESCRIPTION_TEMPLATE_ID }}
          
          # Clinic-Specific Frontend URLs (Multi-Tenant)
          # ============================================
          # ARCHITECTURE: Single Backend API, Multiple Frontends
          # - Each clinic has its own frontend URL (e.g., https://vishwamurti.viddhakarma.com)
          # - All frontends connect to the SAME backend API (API_URL)
          # - Each frontend automatically sends X-Clinic-ID header in all requests
          # - Backend uses X-Clinic-ID to identify which clinic the request is for
          #
          # IMPORTANT: All clinic frontend URLs MUST be added to CORS_ORIGIN variable
          # Format: Comma-separated list (no spaces after commas)
          # Example: "https://ishswami.in,https://vishwamurti.viddhakarma.com,https://clinic2.viddhakarma.com"
          #
          # Pattern: CLINIC_{SANITIZED_CLINIC_NAME}_FRONTEND_URL
          # Example: "Vishwamurti Ayurvedelay" â†’ "VISHWAMURTI_AYURVEDELAY"
          #
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example for "Aadesh Ayurvedalay":
          # CLINIC_AADESH_AYURVEDELAY_FRONTEND_URL=${{ vars.CLINIC_AADESH_AYURVEDELAY_FRONTEND_URL }}
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_FRONTEND_URL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FRONTEND_URL }}
          
          VIDEO_ENABLED=${{ vars.VIDEO_ENABLED }}
          VIDEO_PROVIDER=${{ vars.VIDEO_PROVIDER }}
          OPENVIDU_URL=${{ vars.OPENVIDU_URL }}
          OPENVIDU_SECRET=${{ secrets.OPENVIDU_SECRET }}
          OPENVIDU_DOMAIN=${{ vars.OPENVIDU_DOMAIN }}
          OPENVIDU_WEBHOOK_ENABLED=${{ vars.OPENVIDU_WEBHOOK_ENABLED }}
          OPENVIDU_WEBHOOK_ENDPOINT=${{ vars.OPENVIDU_WEBHOOK_ENDPOINT }}
          OPENVIDU_WEBHOOK_EVENTS=${{ vars.OPENVIDU_WEBHOOK_EVENTS }}
          GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}
          GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}
          GOOGLE_REDIRECT_URI=${{ vars.GOOGLE_REDIRECT_URI }}
          SESSION_SECRET=${{ secrets.SESSION_SECRET }}
          SESSION_TIMEOUT=${{ secrets.SESSION_TIMEOUT }}
          SESSION_SECURE_COOKIES=${{ vars.SESSION_SECURE_COOKIES }}
          SESSION_SAME_SITE=${{ vars.SESSION_SAME_SITE }}
          COOKIE_SECRET=${{ secrets.COOKIE_SECRET }}
          FIREBASE_PROJECT_ID=${{ vars.FIREBASE_PROJECT_ID }}
          FIREBASE_PRIVATE_KEY=${{ secrets.FIREBASE_PRIVATE_KEY }}
          FIREBASE_CLIENT_EMAIL=${{ vars.FIREBASE_CLIENT_EMAIL }}
          FIREBASE_DATABASE_URL=${{ vars.FIREBASE_DATABASE_URL }}
          FIREBASE_VAPID_KEY=${{ secrets.FIREBASE_VAPID_KEY }}
          
          # Clinic-Specific Firebase Configuration (Multi-Tenant)
          # ======================================================
          # Each clinic can have separate Firebase credentials for push notifications
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_PROJECT_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_PROJECT_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_PRIVATE_KEY=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_PRIVATE_KEY }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_CLIENT_EMAIL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_CLIENT_EMAIL }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_DATABASE_URL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_DATABASE_URL }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_VAPID_KEY=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_VAPID_KEY }}
          
          # Clinic-Specific SMS Configuration (Multi-Tenant)
          # ================================================
          # Each clinic can have separate SMS provider credentials
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_API_KEY=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_API_KEY }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_API_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_API_SECRET }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_FROM_NUMBER=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_FROM_NUMBER }}
          
          # Clinic-Specific OpenVidu Configuration (Multi-Tenant)
          # ====================================================
          # Each clinic can have separate OpenVidu video server configuration
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_URL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_URL }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_SECRET }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_DOMAIN=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_DOMAIN }}
          
          # Clinic-Specific S3 Storage Configuration (Multi-Tenant)
          # =======================================================
          # Each clinic can have separate S3 storage bucket and credentials
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_S3_BUCKET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_S3_BUCKET }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_S3_ACCESS_KEY_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_S3_ACCESS_KEY_ID }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_S3_SECRET_ACCESS_KEY=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_S3_SECRET_ACCESS_KEY }}
          
          # Clinic-Specific Social Auth Configuration (Multi-Tenant)
          # ========================================================
          # Each clinic can have separate OAuth credentials (Google, Facebook, Apple)
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_GOOGLE_CLIENT_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_GOOGLE_CLIENT_ID }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_GOOGLE_CLIENT_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_GOOGLE_CLIENT_SECRET }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_FACEBOOK_APP_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FACEBOOK_APP_ID }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_FACEBOOK_APP_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FACEBOOK_APP_SECRET }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_APPLE_CLIENT_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_APPLE_CLIENT_ID }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_APPLE_CLIENT_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_APPLE_CLIENT_SECRET }}
          
          FACEBOOK_APP_ID=${{ vars.FACEBOOK_APP_ID }}
          FACEBOOK_APP_SECRET=${{ vars.FACEBOOK_APP_SECRET }}
          APPLE_CLIENT_ID=${{ vars.APPLE_CLIENT_ID }}
          APPLE_CLIENT_SECRET=${{ vars.APPLE_CLIENT_SECRET }}
          S3_ENABLED=${{ vars.S3_ENABLED }}
          S3_PROVIDER=${{ vars.S3_PROVIDER }}
          S3_ENDPOINT=${{ vars.S3_ENDPOINT }}
          S3_REGION=${{ vars.S3_REGION }}
          S3_BUCKET=${{ vars.S3_BUCKET }}
          S3_ACCESS_KEY_ID=${{ secrets.S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY=${{ secrets.S3_SECRET_ACCESS_KEY }}
          S3_FORCE_PATH_STYLE=${{ vars.S3_FORCE_PATH_STYLE }}
          S3_PUBLIC_URL_EXPIRATION=${{ vars.S3_PUBLIC_URL_EXPIRATION }}
          CDN_URL=${{ vars.CDN_URL }}
          DOCKER_ENV=${{ vars.DOCKER_ENV }}
          DOCKER_NETWORK=${{ vars.DOCKER_NETWORK }}
          EOF

      - name: Copy files to server
        run: |
          # Copy all deployment scripts
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts -r devops/scripts/shared devops/scripts/docker-infra ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/opt/healthcare-backend/devops/scripts/
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts /tmp/.env.production ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/.env.production
          # Copy docker-compose file
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/docker/docker-compose.prod.yml ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/docker-compose.prod.yml

      - name: Set deployment image name
        id: deploy-image
        run: |
          REPO_LOWER=$(echo "${{ env.IMAGE_REPO }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_FULL="ghcr.io/${REPO_LOWER}/${{ env.IMAGE_NAME }}"
          echo "image=${IMAGE_FULL}" >> $GITHUB_OUTPUT
          echo "Image: ${IMAGE_FULL}"
      
      - name: Validate both infrastructure and app images
        if: always()
        run: |
          echo "ðŸ” Validating infrastructure and app images..."
          
          # Validate app image exists in registry
          APP_IMAGE="${{ steps.deploy-image.outputs.image }}:main-${{ github.sha }}"
          echo "Checking app image: ${APP_IMAGE}"
          docker manifest inspect "${APP_IMAGE}" > /dev/null 2>&1 || {
            echo "âŒ App image not found: ${APP_IMAGE}"
            exit 1
          }
          echo "âœ… App image validated: ${APP_IMAGE}"
          
          # Note: Infrastructure images are pulled from docker-compose during deployment
          # They are validated when docker-compose pulls them
          echo "âœ… Infrastructure images will be validated during deployment"
          echo "âœ… Both infrastructure and app images will be checked"

      - name: Setup server directories
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            set -e
            
            # Run directory setup script
            if [[ -f /opt/healthcare-backend/devops/scripts/docker-infra/setup-directories.sh ]]; then
              echo "Setting up server directories..."
              chmod +x /opt/healthcare-backend/devops/scripts/docker-infra/setup-directories.sh
              /opt/healthcare-backend/devops/scripts/docker-infra/setup-directories.sh
            else
              echo "Setup script not found, creating directories manually..."
              mkdir -p /opt/healthcare-backend/backups/{postgres,dragonfly,metadata}
              mkdir -p /opt/healthcare-backend/data/{postgres,dragonfly,openvidu_recordings}
              # /var/log/deployments requires sudo if not running as root
              if [[ "$EUID" -eq 0 ]]; then
                mkdir -p /var/log/deployments
                chmod 755 /var/log/deployments
              else
                sudo mkdir -p /var/log/deployments
                sudo chmod 755 /var/log/deployments
              fi
              chmod 700 /opt/healthcare-backend/backups
              chmod 755 /opt/healthcare-backend/data
              echo "Directories created"
            fi
          ENDSSH

      - name: Deploy to server via SSH
        env:
          SERVER_DEPLOY_PATH: ${{ vars.SERVER_DEPLOY_PATH }}
          IMAGE_TAG: main-${{ github.sha }}
          REGISTRY: ${{ env.REGISTRY }}
          IMAGE: ${{ steps.deploy-image.outputs.image }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_USERNAME: ${{ github.actor }}
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            set -e
            
            # Set deployment variables (values embedded from GitHub Actions)
            # Handle SERVER_DEPLOY_PATH with default value and input validation
            # Security: Validate SERVER_DEPLOY_PATH to prevent path traversal attacks
            SERVER_DEPLOY_PATH_SECRET_VALUE="${{ vars.SERVER_DEPLOY_PATH }}"
            
            # Input validation: Only allow alphanumeric, forward slashes, hyphens, and underscores
            # Prevent path traversal and command injection
            if [ -n "${SERVER_DEPLOY_PATH_SECRET_VALUE}" ] && [ "${SERVER_DEPLOY_PATH_SECRET_VALUE}" != "" ]; then
              # Validate path format (no .., no spaces, no special chars except /, -, _)
              if [[ ! "${SERVER_DEPLOY_PATH_SECRET_VALUE}" =~ ^[a-zA-Z0-9/_-]+$ ]] || [[ "${SERVER_DEPLOY_PATH_SECRET_VALUE}" == *".."* ]]; then
                echo "âŒ ERROR: Invalid SERVER_DEPLOY_PATH format (security check failed)"
                exit 1
              fi
              export SERVER_DEPLOY_PATH="${SERVER_DEPLOY_PATH_SECRET_VALUE}"
              echo "âœ… Using SERVER_DEPLOY_PATH from secret: '${SERVER_DEPLOY_PATH}'"
            else
              export SERVER_DEPLOY_PATH="/opt/healthcare-backend"
              echo "âœ… Using default SERVER_DEPLOY_PATH: '${SERVER_DEPLOY_PATH}'"
            fi
            
            # Final safety check - should never be empty at this point
            if [ -z "${SERVER_DEPLOY_PATH}" ] || [ "${SERVER_DEPLOY_PATH}" = "" ]; then
              echo "âŒ ERROR: SERVER_DEPLOY_PATH is empty! Forcing to default..."
              export SERVER_DEPLOY_PATH="/opt/healthcare-backend"
            fi
            
            # One final verification
            if [ -z "${SERVER_DEPLOY_PATH}" ]; then
              echo "âŒ FATAL: SERVER_DEPLOY_PATH is still empty after all attempts!"
              exit 1
            fi
            
            # Security: Don't log full path in production logs (masked by GitHub Actions)
            echo "ðŸ“ Final SERVER_DEPLOY_PATH='[REDACTED]'"
            export IMAGE_TAG="main-${{ github.sha }}"
            export REGISTRY="${{ env.REGISTRY }}"
            export IMAGE="${{ steps.deploy-image.outputs.image }}"
            export GITHUB_TOKEN="${{ secrets.GITHUB_TOKEN }}"
            export GITHUB_USERNAME="${{ github.actor }}"
            
            # Debug: Show deployment info (secrets masked by GitHub Actions)
            echo "Deploying image: ${IMAGE}:${IMAGE_TAG}"
            # Security: Don't log full path in production logs
            echo "Server deployment path: [REDACTED]"
            
            # Validate IMAGE is set
            if [ -z "${IMAGE}" ] || [ "${IMAGE}" = "ghcr.io/your-username/your-repo/healthcare-api" ]; then
              echo "ERROR: IMAGE environment variable is not set or invalid"
              echo "IMAGE value: '${IMAGE}'"
              exit 1
            fi
            
            # Validate SERVER_DEPLOY_PATH is set (should never be empty after our check above)
            if [ -z "${SERVER_DEPLOY_PATH}" ]; then
              echo "âŒ ERROR: SERVER_DEPLOY_PATH is empty after default assignment!"
              echo "This should never happen. Check the logic above."
              exit 1
            fi
            
            # Create deployment directory if it doesn't exist (deploy.sh expects this)
            # Use SERVER_DEPLOY_PATH directly - no intermediate variable needed
            # Verify it's still set before using it
            if [ -z "${SERVER_DEPLOY_PATH}" ]; then
              echo "âŒ ERROR: SERVER_DEPLOY_PATH is empty when trying to create directory!"
              exit 1
            fi
            
            echo "ðŸ“ Creating deployment directory: ${SERVER_DEPLOY_PATH}"
            mkdir -p "${SERVER_DEPLOY_PATH}/devops/docker" || {
              echo "âŒ ERROR: Failed to create directory: ${SERVER_DEPLOY_PATH}/devops/docker"
              exit 1
            }
            
            echo "ðŸ“„ Moving .env.production to ${SERVER_DEPLOY_PATH}/.env.production"
            mv /tmp/.env.production "${SERVER_DEPLOY_PATH}/.env.production" || {
              echo "âŒ ERROR: Failed to move .env.production to ${SERVER_DEPLOY_PATH}/.env.production"
              exit 1
            }
            chmod 600 "${SERVER_DEPLOY_PATH}/.env.production"
            
            echo "ðŸ“„ Moving docker-compose.prod.yml to ${SERVER_DEPLOY_PATH}/devops/docker/docker-compose.prod.yml"
            mv /tmp/docker-compose.prod.yml "${SERVER_DEPLOY_PATH}/devops/docker/docker-compose.prod.yml" || {
              echo "âŒ ERROR: Failed to move docker-compose.prod.yml"
              exit 1
            }
            
            # Make scripts executable
            chmod +x /opt/healthcare-backend/devops/scripts/docker-infra/*.sh
            chmod +x /opt/healthcare-backend/devops/scripts/shared/*.sh
            
            # Set deployment variables for deploy.sh
            export INFRA_CHANGED="${{ needs.detect-changes.outputs.infra-changed }}"
            export APP_CHANGED="${{ needs.detect-changes.outputs.app-changed }}"
            export INFRA_HEALTHY="${{ needs.check-infrastructure.outputs.infra-healthy }}"
            export INFRA_STATUS="${{ needs.check-infrastructure.outputs.infra-status }}"
            export BACKUP_ID="${{ needs.backup-infrastructure.outputs.backup-id || '' }}"
            
            # Flag to indicate infrastructure operations were already handled by CI/CD jobs
            # This prevents deploy.sh from duplicating backup/recreate/restore operations
            if [[ "${{ needs.detect-changes.outputs.infra-changed }}" == "true" ]] || \
               [[ "${{ needs.check-infrastructure.outputs.infra-healthy }}" == "false" ]]; then
              export INFRA_ALREADY_HANDLED="true"
            else
              export INFRA_ALREADY_HANDLED="false"
            fi
            
            # Run smart deployment orchestrator
            # Note: deploy.sh will skip infrastructure operations if INFRA_ALREADY_HANDLED=true
            # and only handle application deployment
            cd /opt/healthcare-backend
            /opt/healthcare-backend/devops/scripts/docker-infra/deploy.sh
            
          ENDSSH

      - name: Wait for containers to start
        if: always()
        run: |
          echo "â³ Waiting for containers to fully start after deployment..."
          echo "   This gives application containers time to initialize"
          sleep 30  # Wait 30 seconds for containers to start and stabilize
          echo "âœ… Wait complete - proceeding to verification"

      - name: Verify deployment
        if: always()
        run: |
          echo "ðŸ¥ Verifying deployment health..."
          sleep 15  # Additional wait for services to fully start
          
          MAX_RETRIES=10
          RETRY_COUNT=0
          HEALTH_URL="https://api.ishswami.in/health"
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -f -s "${HEALTH_URL}" > /dev/null 2>&1; then
              echo "âœ… Deployment health check passed!"
              exit 0
            fi
            
            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "â³ Waiting for health check... (${RETRY_COUNT}/${MAX_RETRIES})"
            sleep 5
          done
          
          echo "âŒ Health check failed after ${MAX_RETRIES} attempts"
          echo "ðŸ”„ Rollback should have been triggered automatically by deploy script"
          exit 1

      - name: Deployment Success
        run: |
          echo "âœ… Deployment completed successfully!"
          echo "Image: ${{ steps.deploy-image.outputs.image }}:main-${{ github.sha }}"
          # Security: Don't log server hostname in public logs
          echo "Server: [REDACTED]"

  # Ensure Infrastructure Health (At Any Cost)
  # Runs independently to make infrastructure healthy regardless of other job status
  ensure-infrastructure-health:
    name: Ensure Infrastructure Health
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main'
    environment: production
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy infrastructure scripts
        run: |
          # Copy to temp files first, then move to final location (prevents "Text file busy" error)
          # This ensures files are fully written and closed before being executed
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/health-check.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-check.sh.tmp
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/deploy.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/deploy.sh.tmp
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/diagnose.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/diagnose.sh.tmp
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh.tmp
          
          # Move temp files to final location atomically
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            mv /tmp/health-check.sh.tmp /tmp/health-check.sh
            mv /tmp/deploy.sh.tmp /tmp/deploy.sh
            mv /tmp/diagnose.sh.tmp /tmp/diagnose.sh
            mv /tmp/utils.sh.tmp /tmp/utils.sh
            chmod +x /tmp/health-check.sh /tmp/deploy.sh /tmp/diagnose.sh
          ENDSSH

      - name: Make infrastructure healthy (aggressive recovery)
        id: ensure-health
        continue-on-error: false
        run: |
          SSH_EXIT=0
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH' || SSH_EXIT=$?
            set -e
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            export AUTO_RECREATE_MISSING="true"
            
            # Explicitly force-recreate Portainer if it's restarting (fixes old command issue)
            if docker ps -a --format '{{.Names}}\t{{.Status}}' 2>/dev/null | grep -q "^portainer.*Restarting"; then
              echo "ðŸ”§ Portainer is restarting - force recreating with correct command..."
              docker stop portainer 2>/dev/null || true
              docker rm -f portainer 2>/dev/null || true
              cd /opt/healthcare-backend/devops/docker 2>/dev/null || cd /opt/healthcare-backend 2>/dev/null || true
              docker compose -f docker-compose.prod.yml --profile infrastructure up -d --force-recreate portainer 2>&1 || true
              sleep 5
            fi
            
            echo "ðŸš¨ Starting aggressive infrastructure health recovery..."
            echo "ðŸ“‹ This job will make infrastructure healthy at ANY COST"
            echo ""
            
            # Maximum retry attempts
            MAX_ATTEMPTS=5
            ATTEMPT=0
            HEALTHY=false
            
            while [[ $ATTEMPT -lt $MAX_ATTEMPTS ]] && [[ "$HEALTHY" != "true" ]]; do
              ATTEMPT=$((ATTEMPT + 1))
              echo ""
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              echo "ðŸ”„ Recovery Attempt $ATTEMPT/$MAX_ATTEMPTS"
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              
              # Step 1: Run health check with auto-recovery
              echo "ðŸ“Š Step 1: Running health check with auto-recovery..."
              
              # Ensure files are fully written before execution (fix "Text file busy" error)
              sync
              sleep 1
              
              if /tmp/health-check.sh > /tmp/health-attempt-${ATTEMPT}.json 2>&1; then
                HEALTHY=true
                echo "âœ… Infrastructure is healthy!"
                # Write success result immediately
                echo "HEALTHY=true" > /tmp/health-result.txt
                echo "STATUS=healthy" >> /tmp/health-result.txt
                break
              else
                HEALTH_EXIT=$?
                echo "âš ï¸  Health check failed (exit code: $HEALTH_EXIT)"
                cat /tmp/health-attempt-${ATTEMPT}.json || true
              fi
              
              # Step 2: Run diagnostics and auto-fix
              if [[ "$HEALTHY" != "true" ]]; then
                echo ""
                echo "ðŸ”§ Step 2: Running diagnostics and auto-fix..."
                if [[ -f /tmp/diagnose.sh ]]; then
                  /tmp/diagnose.sh > /tmp/diagnose-attempt-${ATTEMPT}.log 2>&1 || true
                  cat /tmp/diagnose-attempt-${ATTEMPT}.log || true
                fi
              fi
              
              # Step 3: Force infrastructure recreation if still unhealthy
              if [[ "$HEALTHY" != "true" ]] && [[ $ATTEMPT -ge 2 ]]; then
                echo ""
                echo "ðŸ”„ Step 3: Force recreating infrastructure (attempt $ATTEMPT)..."
                cd /opt/healthcare-backend/devops/docker || cd /tmp || true
                
                # Stop all infrastructure containers
                echo "â¹ï¸  Stopping infrastructure containers..."
                docker compose -f docker-compose.prod.yml --profile infrastructure down --timeout 30 || true
                
                # Remove unhealthy containers
                echo "ðŸ—‘ï¸  Removing unhealthy containers..."
                docker ps -a --filter "name=postgres|dragonfly|coturn|portainer|openvidu" --format "{{.Names}}" | xargs -r docker rm -f || true
                
                # Recreate infrastructure
                echo "ðŸš€ Recreating infrastructure..."
                docker compose -f docker-compose.prod.yml --profile infrastructure up -d --force-recreate --remove-orphans || {
                  echo "âš ï¸  Docker compose failed, trying individual services..."
                  
                  # Try to start services individually
                  for service in postgres dragonfly coturn portainer openvidu-server; do
                    echo "  â†’ Starting $service..."
                    docker compose -f docker-compose.prod.yml --profile infrastructure up -d $service || true
                  done
                }
                
                # Wait for services to start
                echo "â³ Waiting for services to stabilize..."
                sleep 30
              fi
              
              # Step 4: Re-check health after recovery
              if [[ "$HEALTHY" != "true" ]]; then
                echo ""
                echo "ðŸ” Step 4: Re-checking health after recovery..."
                sleep 10
                
                if /tmp/health-check.sh > /tmp/health-after-recovery-${ATTEMPT}.json 2>&1; then
                  HEALTHY=true
                  echo "âœ… Infrastructure is now healthy after recovery!"
                  break
                else
                  echo "âš ï¸  Still unhealthy after recovery attempt $ATTEMPT"
                  cat /tmp/health-after-recovery-${ATTEMPT}.json || true
                fi
              fi
              
              # Wait before next attempt (exponential backoff)
              if [[ "$HEALTHY" != "true" ]] && [[ $ATTEMPT -lt $MAX_ATTEMPTS ]]; then
                WAIT_TIME=$((ATTEMPT * 15))
                echo ""
                echo "â³ Waiting ${WAIT_TIME}s before next attempt..."
                sleep $WAIT_TIME
              fi
            done
            
            # Final health check
            echo ""
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "ðŸ Final Health Check"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            
            # Ensure files are fully written before execution (fix "Text file busy" error)
            sync
            sleep 2
            
            if /tmp/health-check.sh > /tmp/final-health.json 2>&1; then
              echo "âœ… SUCCESS: Infrastructure is healthy!"
              echo "HEALTHY=true" > /tmp/health-result.txt
              echo "STATUS=healthy" >> /tmp/health-result.txt
              cat /tmp/final-health.json
              exit 0
            else
              FINAL_EXIT=$?
              echo "âŒ FAILURE: Infrastructure is still unhealthy after $MAX_ATTEMPTS attempts"
              echo "HEALTHY=false" > /tmp/health-result.txt
              echo "STATUS=unhealthy" >> /tmp/health-result.txt
              echo "EXIT_CODE=${FINAL_EXIT}" >> /tmp/health-result.txt
              cat /tmp/final-health.json || true
              
              # Show container status
              echo ""
              echo "ðŸ“Š Current Container Status:"
              docker ps -a --filter "name=postgres|dragonfly|coturn|portainer|openvidu" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" || true
              
              echo ""
              echo "ðŸ“‹ Container Logs (last 20 lines):"
              for container in postgres dragonfly coturn portainer openvidu-server; do
                echo "--- $container ---"
                docker logs --tail 20 $container 2>&1 || true
              done
              
              exit 1
            fi
          ENDSSH
          
          # Download results
          # Note: The SSH command above exits with 0 if health check passed, 1 if failed
          # We need to capture the SSH exit code, but since it's in a heredoc, we check the result file
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-result.txt /tmp/health-result.txt 2>&1 || SCP_FAILED=true
          
          if [[ -f /tmp/health-result.txt ]]; then
            HEALTHY=$(grep "^HEALTHY=" /tmp/health-result.txt | cut -d= -f2 || echo "false")
            STATUS=$(grep "^STATUS=" /tmp/health-result.txt | cut -d= -f2 || echo "unknown")
            echo "infra-healthy=${HEALTHY}" >> $GITHUB_OUTPUT
            echo "infra-status=${STATUS}" >> $GITHUB_OUTPUT
            echo "âœ… Downloaded health result: HEALTHY=${HEALTHY}, STATUS=${STATUS}"
            
            # If health check failed according to file, fail this step
            if [[ "${HEALTHY}" != "true" ]]; then
              echo "âŒ Health check failed according to result file"
              exit 1
            fi
          else
            # File not found - this could mean:
            # 1. SSH command failed (health check failed) - step would have already failed
            # 2. File wasn't written or scp failed - but SSH succeeded, so assume healthy
            if [[ "${SCP_FAILED:-false}" == "true" ]]; then
              echo "âš ï¸  Could not download health-result.txt, but SSH command succeeded - assuming healthy"
              echo "infra-healthy=true" >> $GITHUB_OUTPUT
              echo "infra-status=healthy" >> $GITHUB_OUTPUT
            else
              # SSH command must have failed (step would exit with error)
              echo "âŒ Health check failed - SSH command exited with non-zero code"
              echo "infra-healthy=false" >> $GITHUB_OUTPUT
              echo "infra-status=unknown" >> $GITHUB_OUTPUT
              exit 1
            fi
          fi

      - name: Display final status
        if: always()
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ðŸ“Š Infrastructure Health Recovery Summary"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "Status: ${{ steps.ensure-health.outputs.infra-status }}"
          echo "Healthy: ${{ steps.ensure-health.outputs.infra-healthy }}"
          echo ""
          
          if [[ "${{ steps.ensure-health.outputs.infra-healthy }}" != "true" ]]; then
            echo "âŒ Infrastructure recovery failed!"
            echo "âš ï¸  Manual intervention may be required"
            exit 1
          else
            echo "âœ… Infrastructure is healthy!"
          fi

  # Summary Report
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [lint, security, docker-build]
    # Allow lint to be skipped for production (main branch)
    if: always() && !contains(needs.*.result, 'failure')
    steps:
      - name: Report Success
        run: |
          echo "âœ… All CI checks passed successfully!"
          if [ "${{ github.ref }}" == "refs/heads/main" ] && [ "${{ github.event_name }}" == "push" ]; then
            echo "ðŸš€ Ready for deployment (deploy job runs separately)"
          else
            echo "Ready for merge to main branch"
          fi
          echo "â„¹ï¸  Linting and formatting handled by git hooks (pre-commit + pre-push) and Docker build"
