# Continuous Integration Workflow - Docker Only
#
# FULL FLOW (push to main only):
#   1. detect-changes (push) â†’ security â†’ docker-build + ensure-infrastructure-health (parallel)
#   2. check-infrastructure (needs docker-build, ensure-infrastructure-health)
#
#   IF HEALTHY (infra-healthy=true):
#      â†’ check-infrastructure SUCCEEDS â†’ deploy runs (backup/recreate/restore/verify SKIPPED)
#
#   IF UNHEALTHY (infra-healthy=false):
#      â†’ check-infrastructure FAILS
#      â†’ backup-infrastructure (backup first)
#      â†’ recreate-infrastructure (recreate ONLY unhealthy services; healthy left running)
#      â†’ restore-backup (if backup-id set; skipped if backup failed)
#      â†’ verify-infrastructure-readiness
#      â†’ deploy (if verify succeeds)
#
# Goal: Deploy app successfully with no infra issues and no API issues. Infra is fixed (recreate only unhealthy)
# before deploy; after deploy we verify infra + API health and fail the pipeline if any issue remains.
# Implementation summary: docs/CI_CD_WORKFLOW_SUMMARY.md
name: CI/CD

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [develop, main]
    paths-ignore:
      - "README.md"
      - "**/*.md"
      - "**/*.svg"

permissions:
  contents: read
  pull-requests: write
  checks: write
  packages: write
  security-events: write # Required for CodeQL SARIF upload
  actions: read # Required for CodeQL to access workflow run information

env:
  NODE_VERSION: "20"
  YARN_VERSION: "1.22.22"
  REGISTRY: ghcr.io
  IMAGE_REPO: ${{ github.repository }}
  IMAGE_NAME: healthcare-api
  # Note: IMAGE will be set in docker-build job with lowercase conversion

jobs:
  # Security Scanning
  # Runs Trivy vulnerability scanner and dependency audit (audit-ci)
  # Uploads SARIF results to GitHub Security
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: [detect-changes]
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: "fs"
          scan-ref: "."
          format: "sarif"
          output: "trivy-results.sarif"

      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v4
        continue-on-error: true # Don't fail if Advanced Security is not enabled
        with:
          sarif_file: "trivy-results.sarif"

      - name: Dependency Check
        run: |
          npx audit-ci --moderate

  # Docker Build & Push
  # Builds and pushes Docker image to GitHub Container Registry
  # Note: Build happens inside Docker, no separate build job needed
  # Prisma client generation happens during Docker build (see Dockerfile line 34)
  # Linting and formatting are done in git hooks (pre-commit + pre-push) and Docker build
  # Pre-push hook runs full yarn build, ensuring only buildable code reaches CI
  docker-build:
    name: Docker Build & Push
    runs-on: ubuntu-latest
    needs: [detect-changes, security]
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Yarn
        run: npm install -g yarn@${{ env.YARN_VERSION }} --force

      - name: Validate Prisma configuration
        run: |
          echo "ðŸ” Validating Prisma schema and config files..."
          test -f src/libs/infrastructure/database/prisma/schema.prisma || (echo "âŒ Schema file not found!" && exit 1)
          test -f src/libs/infrastructure/database/prisma/prisma.config.js || (echo "âŒ Config file not found!" && exit 1)
          echo "âœ… Prisma schema and config files found"
          echo ""
          echo "ðŸ” Validating Prisma schema syntax..."
          yarn install --frozen-lockfile --ignore-scripts --silent
          export DATABASE_URL="postgresql://user:password@localhost:5432/dbname"
          yarn prisma format --schema=./src/libs/infrastructure/database/prisma/schema.prisma --config=./src/libs/infrastructure/database/prisma/prisma.config.js || (echo "âŒ Prisma schema validation failed!" && exit 1)
          echo "âœ… Prisma schema is valid"

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Set image name (lowercase)
        id: image
        run: |
          REPO_LOWER=$(echo "${{ env.IMAGE_REPO }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_FULL="ghcr.io/${REPO_LOWER}/${{ env.IMAGE_NAME }}"
          echo "image=${IMAGE_FULL}" >> $GITHUB_OUTPUT
          echo "Image name: ${IMAGE_FULL}"

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.image.outputs.image }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: devops/docker/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Ensure expected tag exists
        run: |
          echo "Ensuring expected deployment tag exists..."
          IMAGE_BASE="${{ steps.image.outputs.image }}"
          EXPECTED_TAG="main-${{ github.sha }}"
          EXPECTED_IMAGE="${IMAGE_BASE}:${EXPECTED_TAG}"

          # Get the first tag from metadata (should be one of the pushed tags)
          # Metadata action outputs tags separated by newlines
          FIRST_TAG=$(echo "${{ steps.meta.outputs.tags }}" | grep -v '^$' | head -n 1 | tr -d ' ')

          if [[ -z "$FIRST_TAG" ]]; then
            echo "âŒ Error: No tags found from metadata action"
            echo "Metadata tags output: ${{ steps.meta.outputs.tags }}"
            exit 1
          fi

          echo "First tag from metadata: ${FIRST_TAG}"
          echo "Expected deployment tag: ${EXPECTED_TAG}"
          echo "All metadata tags:"
          echo "${{ steps.meta.outputs.tags }}" | while read -r tag; do
            [[ -n "$tag" ]] && echo "  - ${tag}"
          done

          # Check if expected tag already exists in the tags list
          if echo "${{ steps.meta.outputs.tags }}" | grep -q "${EXPECTED_TAG}"; then
            echo "âœ… Expected tag ${EXPECTED_TAG} was already created by metadata action"
          else
            echo "âš ï¸  Expected tag ${EXPECTED_TAG} not in metadata tags, creating it..."
            # Ensure we're still logged in to GHCR
            echo "${{ secrets.GITHUB_TOKEN }}" | docker login "${{ env.REGISTRY }}" -u "${{ github.actor }}" --password-stdin || {
              echo "âŒ Failed to authenticate with GHCR"
              exit 1
            }
            # Pull the image with one of the existing tags
            docker pull "${FIRST_TAG}" || {
              echo "âŒ Failed to pull image with tag ${FIRST_TAG}"
              exit 1
            }
            # Tag it with the expected tag
            docker tag "${FIRST_TAG}" "${EXPECTED_IMAGE}" || {
              echo "âŒ Failed to tag image"
              exit 1
            }
            # Push the expected tag
            docker push "${EXPECTED_IMAGE}" || {
              echo "âŒ Failed to push expected tag"
              exit 1
            }
            echo "âœ… Created and pushed expected tag: ${EXPECTED_IMAGE}"
          fi

      - name: Verify image was pushed
        run: |
          echo "Verifying pushed image tags..."
          IMAGE_BASE="${{ steps.image.outputs.image }}"
          EXPECTED_TAG="main-${{ github.sha }}"
          EXPECTED_IMAGE="${IMAGE_BASE}:${EXPECTED_TAG}"

          echo "Expected image: ${EXPECTED_IMAGE}"
          echo "All tags from metadata: ${{ steps.meta.outputs.tags }}"

          # Wait a moment for registry propagation
          sleep 5

          # Verify the SHA-based tag exists
          if docker manifest inspect "${EXPECTED_IMAGE}" > /dev/null 2>&1; then
            echo "âœ… Verified: ${EXPECTED_IMAGE} exists in registry"
          else
            echo "âš ï¸  Warning: ${EXPECTED_IMAGE} not found in registry yet (may need propagation time)"
            echo "Checking :latest tag as fallback..."
            if docker manifest inspect "${IMAGE_BASE}:latest" > /dev/null 2>&1; then
              echo "âœ… Verified: ${IMAGE_BASE}:latest exists in registry (will be used as fallback)"
            else
              echo "âŒ Error: Neither ${EXPECTED_IMAGE} nor ${IMAGE_BASE}:latest found in registry"
              echo "This indicates the build/push may have failed"
              exit 1
            fi
          fi

      - name: Output image digest
        run: |
          echo "Image pushed: ${{ steps.meta.outputs.tags }}"
          echo "Expected deployment tag: main-${{ github.sha }}"

  # Change Detection
  # Detects whether infrastructure or application code has changed
  # Outputs: infra-changed, app-changed
  # Used by other jobs to determine if infrastructure/app deployment is needed
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    outputs:
      infra-changed: ${{ steps.filter.outputs.infra }}
      app-changed: ${{ steps.filter.outputs.app }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Detect infrastructure and application changes
        uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            infra:
              - 'devops/docker/Dockerfile.postgres'
              - 'devops/docker/Dockerfile.dragonfly'
              - 'devops/docker/Dockerfile.openvidu'
              - 'devops/docker/docker-compose.prod.yml'
              - 'src/libs/infrastructure/database/prisma/migrations/**'
            app:
              - 'src/**'
              - 'package.json'
              - 'yarn.lock'
              - 'tsconfig.json'
              - 'devops/docker/Dockerfile'

      - name: Output changes
        run: |
          echo "Infrastructure changed: ${{ steps.filter.outputs.infra }}"
          echo "Application changed: ${{ steps.filter.outputs.app }}"

  # Ensure Infrastructure Health
  # Runs in parallel with docker-build to check infrastructure health early
  # This allows parallel execution: docker-build + ensure-infrastructure-health
  # Outputs: infra-healthy, infra-status
  ensure-infrastructure-health:
    name: Ensure Infrastructure Health
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment: production
    needs: [detect-changes, security]
    timeout-minutes: 10
    outputs:
      infra-healthy: ${{ steps.health-check.outputs.infra-healthy }}
      infra-status: ${{ steps.health-check.outputs.infra-status }}
      unhealthy-services: ${{ steps.health-check.outputs.unhealthy-services }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy health check script
        run: |
          # Copy to temp files first, then move to final location (prevents "Text file busy" error)
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/health-check.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-check.sh.tmp
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh.tmp

          # Move temp files to final location atomically
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            mv /tmp/health-check.sh.tmp /tmp/health-check.sh
            mv /tmp/utils.sh.tmp /tmp/utils.sh
            chmod +x /tmp/health-check.sh /tmp/utils.sh
          ENDSSH

      - name: Run health check
        id: health-check
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            chmod +x /tmp/health-check.sh /tmp/utils.sh
            export SCRIPT_DIR="/tmp"
            export AUTO_RECREATE_MISSING="true"
            # Do not source utils.sh here: it sets "set -e", so when health-check.sh exits 2
            # (EXIT_CRITICAL) the shell would exit before writing EXIT_CODE. health-check.sh
            # sources /tmp/utils.sh itself when it runs.
            sync
            sleep 1
            /tmp/health-check.sh > /tmp/health-status.json 2>&1
            HC_EXIT=$?
            echo "EXIT_CODE=${HC_EXIT}" > /tmp/health-exit.txt
            exit 0
          ENDSSH

          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-status.json /tmp/health-status.json || true
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-exit.txt /tmp/health-exit.txt || true

          if [[ -f /tmp/health-exit.txt ]]; then
            EXIT_CODE=$(cat /tmp/health-exit.txt | grep EXIT_CODE | cut -d= -f2)
            if [[ "$EXIT_CODE" == "0" ]]; then
              echo "infra-healthy=true" >> $GITHUB_OUTPUT
              echo "infra-status=healthy" >> $GITHUB_OUTPUT
              echo "unhealthy-services=" >> $GITHUB_OUTPUT
            else
              echo "infra-healthy=false" >> $GITHUB_OUTPUT
              if [[ "$EXIT_CODE" == "3" ]]; then
                echo "infra-status=missing" >> $GITHUB_OUTPUT
              else
                echo "infra-status=unhealthy" >> $GITHUB_OUTPUT
              fi
              # Extract unhealthy service names from health-status.json; map openvidu -> openvidu-server for compose
              if [[ -f /tmp/health-status.json ]]; then
                UNHEALTHY=$(jq -r '.services | to_entries | map(select(.value.status != "healthy")) | .[].key' /tmp/health-status.json 2>/dev/null || true)
                COMPOSE_SERVICES=""
                for s in $UNHEALTHY; do
                  [[ "$s" == "openvidu" ]] && s="openvidu-server"
                  [[ -n "$COMPOSE_SERVICES" ]] && COMPOSE_SERVICES="${COMPOSE_SERVICES},"
                  COMPOSE_SERVICES="${COMPOSE_SERVICES}${s}"
                done
                echo "unhealthy-services=${COMPOSE_SERVICES}" >> $GITHUB_OUTPUT
              else
                echo "unhealthy-services=" >> $GITHUB_OUTPUT
              fi
            fi
          else
            echo "infra-healthy=false" >> $GITHUB_OUTPUT
            echo "infra-status=unknown" >> $GITHUB_OUTPUT
            echo "unhealthy-services=" >> $GITHUB_OUTPUT
          fi

      - name: Display health status
        run: |
          echo "Infrastructure Health: ${{ steps.health-check.outputs.infra-healthy }}"
          echo "Infrastructure Status: ${{ steps.health-check.outputs.infra-status }}"
          if [[ -f /tmp/health-status.json ]]; then
            cat /tmp/health-status.json
          fi

      - name: Fail job if infrastructure is unhealthy
        if: steps.health-check.outputs.infra-healthy != 'true'
        run: |
          echo "One or more infrastructure services are unhealthy - failing job."
          exit 1

  # Infrastructure Health Check
  # Passes through infra-healthy (true/false). FAILS the job when unhealthy so UI shows red and recovery runs.
  # When infra-healthy=true  â†’ Job succeeds (green) â†’ Backup/Recreate/Restore/Verify SKIPPED â†’ Deploy runs.
  # When infra-healthy=false â†’ Job FAILS (red) â†’ Backup â†’ Recreate â†’ Restore â†’ Verify â†’ Deploy run.
  # Run even when ensure-infrastructure-health failed (so we have outputs and can run recovery).
  check-infrastructure:
    name: Check Infrastructure Health
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      (needs.ensure-infrastructure-health.result == 'success' || needs.ensure-infrastructure-health.result == 'failure')
    environment: production
    needs: [docker-build, ensure-infrastructure-health]
    timeout-minutes: 10
    outputs:
      infra-healthy: ${{ needs.ensure-infrastructure-health.outputs.infra-healthy }}
      infra-status: ${{ needs.ensure-infrastructure-health.outputs.infra-status }}
      unhealthy-services: ${{ needs.ensure-infrastructure-health.outputs.unhealthy-services }}
    steps:
      - name: Display infrastructure health status
        run: |
          echo "=============================================="
          echo "Infra health result: ${{ needs.ensure-infrastructure-health.outputs.infra-healthy }}"
          echo "Infra status: ${{ needs.ensure-infrastructure-health.outputs.infra-status }}"
          echo "Unhealthy services (recreate only these): ${{ needs.ensure-infrastructure-health.outputs.unhealthy-services }}"
          echo "=============================================="
          if [[ "${{ needs.ensure-infrastructure-health.outputs.infra-healthy }}" == "true" ]]; then
            echo "RESULT: HEALTHY â†’ Job will succeed; Backup/Recreate/Restore/Verify SKIPPED; Deploy will run."
          else
            echo "RESULT: UNHEALTHY â†’ Job will FAIL (red); Backup â†’ Recreate only unhealthy â†’ Restore â†’ Verify â†’ Deploy will run."
          fi
          echo "=============================================="

      - name: Fail job when infra is unhealthy
        run: |
          if [[ "${{ needs.ensure-infrastructure-health.outputs.infra-healthy }}" != "true" ]]; then
            echo "Infrastructure unhealthy - failing so recovery runs: backup â†’ recreate only unhealthy â†’ restore â†’ verify â†’ deploy."
            exit 1
          fi

  # Backup Infrastructure (RECOVERY FLOW ONLY)
  # Runs first when infra is unhealthy. Backup is used to restore after we recreate only unhealthy services.
  # If backup step fails (e.g. postgres restarting), job still completes so recreate can run; restore will be skipped.
  backup-infrastructure:
    name: Backup Infrastructure
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      needs.check-infrastructure.result == 'failure'
    environment: production
    needs: [detect-changes, check-infrastructure]
    timeout-minutes: 30
    outputs:
      backup-id: ${{ steps.backup.outputs.backup-id }}
    steps:
      - name: Confirm running because infra is unhealthy
        run: |
          echo "This job runs ONLY when Check Infrastructure Health reported unhealthy (infra-healthy=false)."
          echo "Current run: check-infrastructure.outputs.infra-healthy = ${{ needs.check-infrastructure.outputs.infra-healthy }}"
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy backup scripts
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/backup.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/backup.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh

      - name: Run backup
        id: backup
        continue-on-error: true
        env:
          S3_ENABLED: ${{ vars.S3_ENABLED }}
          S3_PROVIDER: ${{ vars.S3_PROVIDER }}
          S3_ENDPOINT: ${{ vars.S3_ENDPOINT }}
          S3_REGION: ${{ vars.S3_REGION }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
          S3_FORCE_PATH_STYLE: ${{ vars.S3_FORCE_PATH_STYLE }}
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            set -e
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            export S3_ENABLED="${{ vars.S3_ENABLED }}"
            export S3_PROVIDER="${{ vars.S3_PROVIDER }}"
            export S3_ENDPOINT="${{ vars.S3_ENDPOINT }}"
            export S3_REGION="${{ vars.S3_REGION }}"
            export S3_BUCKET="${{ vars.S3_BUCKET }}"
            export S3_ACCESS_KEY_ID="${{ secrets.S3_ACCESS_KEY_ID }}"
            export S3_SECRET_ACCESS_KEY="${{ secrets.S3_SECRET_ACCESS_KEY }}"
            export S3_FORCE_PATH_STYLE="${{ vars.S3_FORCE_PATH_STYLE }}"
            
            # Install rclone (Contabo's recommended tool) or s3cmd as fallback
            # Contabo recommends rclone: https://help.contabo.com/en/support/solutions/articles/103000305592-what-is-the-rclone-tool-
            if ! command -v rclone &> /dev/null && ! command -v s3cmd &> /dev/null; then
              echo "ðŸ“¦ Installing rclone for Contabo S3 (Contabo's recommended tool)..."
              # Install rclone using official installer (recommended by Contabo)
              if curl -fsSL https://rclone.org/install.sh | sudo bash; then
                echo "âœ… rclone installed successfully"
              else
                echo "âš ï¸  rclone installation failed, trying s3cmd fallback..."
                if command -v apt-get &> /dev/null; then
                  sudo apt-get update -qq
                  sudo apt-get install -y s3cmd || {
                    echo "âš ï¸  apt-get install failed, trying pip3..."
                    if command -v pip3 &> /dev/null; then
                      pip3 install --user s3cmd || {
                        echo "âŒ Failed to install both rclone and s3cmd"
                        exit 1
                      }
                      export PATH="$HOME/.local/bin:$PATH"
                    else
                      echo "âŒ Cannot install rclone or s3cmd automatically"
                      exit 1
                    fi
                  }
                elif command -v pip3 &> /dev/null; then
                  pip3 install --user s3cmd || {
                    echo "âŒ Failed to install both rclone and s3cmd"
                    exit 1
                  }
                  export PATH="$HOME/.local/bin:$PATH"
                else
                  echo "âŒ Cannot install rclone or s3cmd automatically"
                  exit 1
                fi
                echo "âœ… s3cmd installed as fallback"
              fi
            elif command -v rclone &> /dev/null; then
              echo "âœ… rclone is already installed"
            elif command -v s3cmd &> /dev/null; then
              echo "âœ… s3cmd is already installed (rclone preferred but s3cmd will work)"
            fi
            
            cd /opt/healthcare-backend/devops/docker || exit 1
            
            # CRITICAL: Ensure containers are running before backup
            echo "ðŸ” Ensuring PostgreSQL and Dragonfly containers are running for backup..."
            
            # Check and start PostgreSQL if needed
            # Note: Using variable for format string to avoid bash brace expansion issues in heredoc
            DOCKER_NAME_FMT='{{.Names}}'
            if ! docker ps --format "$DOCKER_NAME_FMT" | grep -q "^postgres$"; then
              echo "âš ï¸  PostgreSQL container not running - starting for backup..."
              docker compose -f docker-compose.prod.yml --profile infrastructure up -d postgres || {
                echo "âŒ Failed to start PostgreSQL for backup"
                exit 1
              }
              echo "â³ Waiting for PostgreSQL to be ready..."
              MAX_RETRIES=30
              RETRY_COUNT=0
              while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                if docker exec postgres pg_isready -U postgres -d userdb >/dev/null 2>&1; then
                  echo "âœ… PostgreSQL is ready for backup"
                  break
                fi
                RETRY_COUNT=$((RETRY_COUNT + 1))
                sleep 2
              done
              if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
                echo "âŒ PostgreSQL did not become ready for backup"
                exit 1
              fi
            else
              echo "âœ… PostgreSQL container is running"
            fi
            
            # Check and start Dragonfly if needed
            if ! docker ps --format "$DOCKER_NAME_FMT" | grep -q "^dragonfly$"; then
              echo "âš ï¸  Dragonfly container not running - starting for backup..."
              docker compose -f docker-compose.prod.yml --profile infrastructure up -d dragonfly || {
                echo "âŒ Failed to start Dragonfly for backup"
                exit 1
              }
              echo "â³ Waiting for Dragonfly to be ready..."
              MAX_RETRIES=20
              RETRY_COUNT=0
              while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                if docker exec dragonfly redis-cli -p 6379 ping >/dev/null 2>&1; then
                  echo "âœ… Dragonfly is ready for backup"
                  break
                fi
                RETRY_COUNT=$((RETRY_COUNT + 1))
                sleep 2
              done
              if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
                echo "âš ï¸  Dragonfly did not become ready (backup may fail for Dragonfly)"
              fi
            else
              echo "âœ… Dragonfly container is running"
            fi
            
            # CRITICAL: Create backup of PostgreSQL and Dragonfly data
            echo "ðŸ’¾ Creating backup of PostgreSQL and Dragonfly data..."
            echo "âš ï¸  This backup is CRITICAL - data will be lost if backup fails!"
            chmod +x /tmp/backup.sh
            BACKUP_ID=$(/tmp/backup.sh pre-deployment) || {
              echo "âŒ Backup failed - ABORTING to prevent data loss"
              exit 1
            }
            echo "âœ… Backup created successfully (ID: ${BACKUP_ID})"
            echo "BACKUP_ID=${BACKUP_ID}" > /tmp/backup-id.txt
          ENDSSH

          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/backup-id.txt /tmp/backup-id.txt || true
          BACKUP_ID=$(grep BACKUP_ID /tmp/backup-id.txt 2>/dev/null | cut -d= -f2 || true)
          echo "backup-id=${BACKUP_ID:-}" >> $GITHUB_OUTPUT
          echo "Backup ID: ${BACKUP_ID:-}(empty if backup step failed)"

  # Recreate Infrastructure (RECOVERY FLOW ONLY)
  # Recreate ONLY unhealthy services/containers; healthy services are never recreated (left running).
  # Flow: backup already ran â†’ clean + recreate only unhealthy â†’ then restore job runs.
  # Volumes kept. Fallback: when unhealthy list is empty we recreate all so deploy can succeed.
  recreate-infrastructure:
    name: Recreate Infrastructure
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      needs.check-infrastructure.result == 'failure' &&
      (needs.backup-infrastructure.result == 'success' || needs.backup-infrastructure.result == 'failure')
    environment: production
    needs: [detect-changes, check-infrastructure, backup-infrastructure]
    timeout-minutes: 20
    steps:
      - name: Recreate mode (only unhealthy; healthy services untouched)
        run: |
          UNHEALTHY="${{ needs.check-infrastructure.outputs.unhealthy-services }}"
          echo "Goal: Deploy app with all services healthy. We recreate ONLY unhealthy containers; healthy ones stay running."
          echo "Flow: Backup (done) â†’ Recreate only unhealthy â†’ Restore backup â†’ Verify â†’ Deploy."
          if [[ -z "${{ needs.backup-infrastructure.outputs.backup-id }}" ]]; then
            echo "No backup-id - restore will be skipped after recreate."
          else
            echo "Backup available (backup-id: ${{ needs.backup-infrastructure.outputs.backup-id }}) - after recreate we will restore."
          fi
          if [[ -n "$UNHEALTHY" ]]; then
            echo "Recreating ONLY these unhealthy services (healthy services will NOT be recreated): $UNHEALTHY"
          else
            echo "Unhealthy list empty (fallback: recreate all infra so we can reach a deployable state)."
          fi
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy docker-compose to server (ensures postgres:18 from repo)
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/docker/docker-compose.prod.yml ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/docker-compose.prod.yml

      - name: Recreate infrastructure containers (only unhealthy services)
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << ENDSSH
            set -e
            cp /tmp/docker-compose.prod.yml /opt/healthcare-backend/devops/docker/docker-compose.prod.yml
            cd /opt/healthcare-backend/devops/docker || exit 1
            UNHEALTHY_SERVICES="${{ needs.check-infrastructure.outputs.unhealthy-services }}"
            
            # Recreate ONLY unhealthy services; healthy services are never touched
            if [[ -n "\${UNHEALTHY_SERVICES}" ]]; then
              echo "ðŸ§¹ Clean then recreate ONLY unhealthy (healthy services left running): \${UNHEALTHY_SERVICES}"
              echo "ðŸ“¥ Pulling infrastructure images first (ensures postgres:18 etc. from docker-compose.prod.yml)..."
              docker compose -f docker-compose.prod.yml --profile infrastructure pull --quiet || true
              if echo "\${UNHEALTHY_SERVICES}" | grep -q "postgres"; then
                echo "ðŸ”§ Fixing PostgreSQL 16â†’18 config (remove invalid autovacuum_worker_slots)..."
                PG_VOL=\$(docker volume ls -q | grep postgres_data | head -1)
                [[ -n "\$PG_VOL" ]] && docker run --rm -v "\${PG_VOL}:/v" alpine sh -c "test -f /v/pgdata/postgresql.conf && sed -i.bak '/autovacuum_worker_slots/d' /v/pgdata/postgresql.conf; exit 0" || true
              fi
              RECREATE_LIST=()
              for s in \$(echo "\${UNHEALTHY_SERVICES}" | tr ',' ' '); do
                s=\$(echo "\$s" | tr -d ' ')
                [[ -n "\$s" ]] && RECREATE_LIST+=("\$s")
              done
              for svc in "\${RECREATE_LIST[@]}"; do
                echo "ðŸ§¹ Cleaning \${svc} (stop + remove)..."
                docker compose -f docker-compose.prod.yml --profile infrastructure stop "\$svc" 2>/dev/null || true
                docker compose -f docker-compose.prod.yml --profile infrastructure rm -f "\$svc" 2>/dev/null || true
                echo "ðŸ”„ Recreating \${svc}..."
                docker compose -f docker-compose.prod.yml --profile infrastructure up -d --force-recreate --no-deps "\$svc" || {
                  echo "âŒ Failed to recreate \${svc}"
                  exit 1
                }
              done
            else
              echo "ðŸ§¹ Clean then recreate: ALL required infrastructure (fallback)"
              echo "ðŸ“‹ Required services (from docker-compose.prod.yml --profile infrastructure): postgres (PostgreSQL 18), dragonfly, openvidu-server, coturn, portainer"
              VOLUMES=("postgres_data" "dragonfly_data")
              VOLUME_PATHS=("/opt/healthcare-backend/data/postgres" "/opt/healthcare-backend/data/dragonfly")
              for i in "\${!VOLUMES[@]}"; do
                [[ ! -d "\${VOLUME_PATHS[\$i]}" ]] && mkdir -p "\${VOLUME_PATHS[\$i]}" || true
              done
              [[ ! -d "/opt/healthcare-backend/data/openvidu_recordings" ]] && mkdir -p /opt/healthcare-backend/data/openvidu_recordings || true
              echo "ðŸ§¹ Cleaning: stopping and removing all infrastructure containers (volumes kept)..."
              docker compose -f docker-compose.prod.yml --profile infrastructure down --remove-orphans || true
              sleep 3
              echo "ðŸ”§ Fixing PostgreSQL 16â†’18 config (remove invalid autovacuum_worker_slots from existing volume)..."
              PG_VOL=\$(docker volume ls -q | grep postgres_data | head -1)
              if [[ -n "\$PG_VOL" ]]; then
                docker run --rm -v "\${PG_VOL}:/v" alpine sh -c "test -f /v/pgdata/postgresql.conf && sed -i.bak '/autovacuum_worker_slots/d' /v/pgdata/postgresql.conf; exit 0" || true
              fi
              echo "ðŸ“¥ Pulling infrastructure images (including postgres:18)..."
              docker compose -f docker-compose.prod.yml --profile infrastructure pull --quiet || true
              echo "ðŸš€ Recreating all infrastructure services (postgres:18, dragonfly, openvidu-server, coturn, portainer)..."
              docker compose -f docker-compose.prod.yml --profile infrastructure up -d --force-recreate || {
                echo "âŒ Infrastructure recreation failed"
                exit 1
              }
            fi
            
            echo "â³ Waiting for containers to stabilize..."
            sleep 15
            
            # Wait for PostgreSQL if it was recreated
            if [[ -z "\${UNHEALTHY_SERVICES}" ]] || echo "\${UNHEALTHY_SERVICES}" | grep -q "postgres"; then
              echo "â³ Waiting for PostgreSQL to be ready..."
              for i in \$(seq 1 30); do
                docker exec postgres pg_isready -U postgres -d userdb >/dev/null 2>&1 && { echo "âœ… PostgreSQL is ready"; break; }
                [[ \$i -eq 30 ]] && { echo "âŒ PostgreSQL did not become ready"; exit 1; }
                sleep 2
              done
            fi
            
            # Wait for Dragonfly if it was recreated
            if [[ -z "\${UNHEALTHY_SERVICES}" ]] || echo "\${UNHEALTHY_SERVICES}" | grep -q "dragonfly"; then
              echo "â³ Waiting for Dragonfly to be ready..."
              for i in \$(seq 1 20); do
                docker exec dragonfly redis-cli -p 6379 ping >/dev/null 2>&1 && { echo "âœ… Dragonfly is ready"; break; }
                [[ \$i -eq 20 ]] && { echo "âŒ Dragonfly did not become ready"; exit 1; }
                sleep 2
              done
            fi
            
            echo "ðŸ“Š Container status:"
            docker compose -f docker-compose.prod.yml --profile infrastructure ps
            echo "âœ… Infrastructure recreation completed successfully"
          ENDSSH

  # Restore Backup (RECOVERY FLOW ONLY)
  # Runs after recreate: restores backup so data is intact. Then verify â†’ deploy.
  # Skipped when no backup-id (e.g. backup failed); pipeline still continues (verify â†’ deploy).
  restore-backup:
    name: Restore Backup
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      needs.check-infrastructure.result == 'failure' &&
      needs.backup-infrastructure.outputs.backup-id != ''
    environment: production
    needs: [check-infrastructure, backup-infrastructure, recreate-infrastructure]
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy restore scripts
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/restore.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/restore.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh

      - name: Restore from backup
        env:
          S3_ENABLED: ${{ vars.S3_ENABLED }}
          S3_PROVIDER: ${{ vars.S3_PROVIDER }}
          S3_ENDPOINT: ${{ vars.S3_ENDPOINT }}
          S3_REGION: ${{ vars.S3_REGION }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
          S3_FORCE_PATH_STYLE: ${{ vars.S3_FORCE_PATH_STYLE }}
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            export S3_ENABLED="${{ vars.S3_ENABLED }}"
            export S3_PROVIDER="${{ vars.S3_PROVIDER }}"
            export S3_ENDPOINT="${{ vars.S3_ENDPOINT }}"
            export S3_REGION="${{ vars.S3_REGION }}"
            export S3_BUCKET="${{ vars.S3_BUCKET }}"
            export S3_ACCESS_KEY_ID="${{ secrets.S3_ACCESS_KEY_ID }}"
            export S3_SECRET_ACCESS_KEY="${{ secrets.S3_SECRET_ACCESS_KEY }}"
            export S3_FORCE_PATH_STYLE="${{ vars.S3_FORCE_PATH_STYLE }}"
            
            # Install rclone (Contabo's recommended tool) or s3cmd as fallback
            if ! command -v rclone &> /dev/null && ! command -v s3cmd &> /dev/null; then
              echo "ðŸ“¦ Installing rclone for Contabo S3 (Contabo's recommended tool)..."
              if curl -fsSL https://rclone.org/install.sh | sudo bash; then
                echo "âœ… rclone installed successfully"
              else
                echo "âš ï¸  rclone installation failed, trying s3cmd fallback..."
                if command -v apt-get &> /dev/null; then
                  sudo apt-get update -qq
                  sudo apt-get install -y s3cmd || {
                    if command -v pip3 &> /dev/null; then
                      pip3 install --user s3cmd && export PATH="$HOME/.local/bin:$PATH" || {
                        echo "âŒ Failed to install both rclone and s3cmd"
                        exit 1
                      }
                    else
                      echo "âŒ Cannot install rclone or s3cmd automatically"
                      exit 1
                    fi
                  }
                elif command -v pip3 &> /dev/null; then
                  pip3 install --user s3cmd && export PATH="$HOME/.local/bin:$PATH" || {
                    echo "âŒ Failed to install both rclone and s3cmd"
                    exit 1
                  }
                else
                  echo "âŒ Cannot install rclone or s3cmd automatically"
                  exit 1
                fi
                echo "âœ… s3cmd installed as fallback"
              fi
            elif command -v rclone &> /dev/null; then
              echo "âœ… rclone is already installed"
            elif command -v s3cmd &> /dev/null; then
              echo "âœ… s3cmd is already installed (rclone preferred but s3cmd will work)"
            fi
            
            chmod +x /tmp/restore.sh
            # Validate backup-id to prevent command injection
            BACKUP_ID="${{ needs.backup-infrastructure.outputs.backup-id }}"
            if [[ ! "$BACKUP_ID" =~ ^[a-zA-Z0-9_-]+$ ]]; then
              echo "ERROR: Invalid backup ID format"
              exit 1
            fi
            /tmp/restore.sh "$BACKUP_ID"
          ENDSSH

  # Verify Infrastructure Readiness (RECOVERY FLOW ONLY)
  # Runs ONLY when check-infrastructure FAILED. Verifies recreated infra is healthy before deploy.
  # Never runs when infra was healthy (check succeeded).
  verify-infrastructure-readiness:
    name: Verify Infrastructure Readiness
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      needs.check-infrastructure.result == 'failure' &&
      (needs.recreate-infrastructure.result == 'success' || needs.recreate-infrastructure.result == 'failure')
    environment: production
    needs:
      [
        check-infrastructure,
        backup-infrastructure,
        recreate-infrastructure,
      ]
    timeout-minutes: 10
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy health check script
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/health-check.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/health-check.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh

      - name: Verify infrastructure readiness
        run: |
          # Run health check and capture exit code
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} 'bash -c "
            export SCRIPT_DIR=\"/tmp\"
            chmod +x /tmp/health-check.sh /tmp/utils.sh 2>/dev/null || true
            
            echo \"ðŸ” Verifying infrastructure readiness before deployment...\"
            echo \"ðŸ“‹ This verifies that newly recreated infrastructure is healthy\"
            
            # Run infrastructure health check
            if /tmp/health-check.sh > /tmp/infra-readiness.json 2>&1; then
              echo \"âœ… Infrastructure is healthy and ready for deployment\"
              exit 0
            else
              EXIT_CODE=\$?
              echo \"âŒ Infrastructure health check failed (exit code: \$EXIT_CODE)\"
              cat /tmp/infra-readiness.json 2>/dev/null || true
              exit 1
            fi
          "'

  # Deploy to Production
  # Runs when: docker-build succeeded AND (check-infrastructure succeeded [healthy] OR verify-infrastructure-readiness succeeded [recovery path]).
  # When check fails (unhealthy), deploy runs only after verify-infrastructure-readiness succeeds.
  deploy:
    name: Deploy to Production
    if: |
      always() &&
      github.event_name == 'push' &&
      github.ref == 'refs/heads/main' &&
      needs.docker-build.result == 'success' &&
      (needs.check-infrastructure.result == 'success' || needs.verify-infrastructure-readiness.result == 'success')
    runs-on: ubuntu-latest
    environment: production # Use production environment secrets
    needs:
      [
        docker-build,
        check-infrastructure,
        verify-infrastructure-readiness,
      ]
    concurrency:
      group: deploy-production
      cancel-in-progress: false
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Create .env.production file
        run: |
          cat > /tmp/.env.production << EOF
          NODE_ENV=${{ vars.NODE_ENV }}
          IS_DEV=${{ vars.IS_DEV }}
          DATABASE_URL=${{ secrets.DATABASE_URL }}
          DIRECT_URL=${{ secrets.DIRECT_URL }}
          DATABASE_SQL_INJECTION_PREVENTION_ENABLED=${{ vars.DATABASE_SQL_INJECTION_PREVENTION_ENABLED }}
          DATABASE_ROW_LEVEL_SECURITY_ENABLED=${{ vars.DATABASE_ROW_LEVEL_SECURITY_ENABLED }}
          DATABASE_DATA_MASKING_ENABLED=${{ vars.DATABASE_DATA_MASKING_ENABLED }}
          DATABASE_RATE_LIMITING_ENABLED=${{ vars.DATABASE_RATE_LIMITING_ENABLED }}
          DATABASE_READ_REPLICAS_ENABLED=${{ vars.DATABASE_READ_REPLICAS_ENABLED }}
          DATABASE_READ_REPLICAS_STRATEGY=${{ vars.DATABASE_READ_REPLICAS_STRATEGY }}
          DATABASE_READ_REPLICAS_URLS=${{ vars.DATABASE_READ_REPLICAS_URLS }}
          CACHE_ENABLED=${{ vars.CACHE_ENABLED }}
          CACHE_PROVIDER=${{ vars.CACHE_PROVIDER }}
          DRAGONFLY_ENABLED=${{ vars.DRAGONFLY_ENABLED }}
          DRAGONFLY_HOST=${{ vars.DRAGONFLY_HOST }}
          DRAGONFLY_PORT=${{ vars.DRAGONFLY_PORT }}
          DRAGONFLY_KEY_PREFIX=${{ vars.DRAGONFLY_KEY_PREFIX }}
          DRAGONFLY_PASSWORD=${{ secrets.DRAGONFLY_PASSWORD }}
          REDIS_HOST=${{ vars.REDIS_HOST }}
          REDIS_PORT=${{ vars.REDIS_PORT }}
          REDIS_TTL=${{ vars.REDIS_TTL }}
          REDIS_PREFIX=${{ vars.REDIS_PREFIX }}
          REDIS_ENABLED=${{ vars.REDIS_ENABLED }}
          REDIS_PASSWORD=${{ vars.REDIS_PASSWORD }}
          PORT=${{ vars.PORT }}
          API_PREFIX=${{ vars.API_PREFIX }}
          HOST=${{ vars.HOST }}
          BIND_ADDRESS=${{ vars.BIND_ADDRESS }}
          BASE_URL=${{ vars.BASE_URL }}
          API_URL=${{ vars.API_URL }}
          FRONTEND_URL=${{ vars.FRONTEND_URL }}
          MAIN_DOMAIN=${{ vars.MAIN_DOMAIN }}
          API_DOMAIN=${{ vars.API_DOMAIN }}
          FRONTEND_DOMAIN=${{ vars.FRONTEND_DOMAIN }}
          JWT_SECRET=${{ secrets.JWT_SECRET }}
          JWT_EXPIRATION=${{ vars.JWT_EXPIRATION }}
          JWT_ACCESS_EXPIRES_IN=${{ vars.JWT_ACCESS_EXPIRES_IN }}
          JWT_REFRESH_EXPIRES_IN=${{ vars.JWT_REFRESH_EXPIRES_IN }}
          JWT_REFRESH_SECRET=${{ secrets.JWT_REFRESH_SECRET }}
          PRISMA_SCHEMA_PATH=${{ vars.PRISMA_SCHEMA_PATH }}
          LOG_LEVEL=${{ vars.LOG_LEVEL }}
          ENABLE_AUDIT_LOGS=${{ vars.ENABLE_AUDIT_LOGS }}
          RATE_LIMIT_ENABLED=${{ vars.RATE_LIMIT_ENABLED }}
          RATE_LIMIT_TTL=${{ vars.RATE_LIMIT_TTL }}
          RATE_LIMIT_MAX=${{ vars.RATE_LIMIT_MAX }}
          API_RATE_LIMIT=${{ vars.API_RATE_LIMIT }}
          AUTH_RATE_LIMIT=${{ vars.AUTH_RATE_LIMIT }}
          HEAVY_RATE_LIMIT=${{ vars.HEAVY_RATE_LIMIT }}
          USER_RATE_LIMIT=${{ vars.USER_RATE_LIMIT }}
          HEALTH_RATE_LIMIT=${{ vars.HEALTH_RATE_LIMIT }}
          MAX_AUTH_ATTEMPTS=${{ vars.MAX_AUTH_ATTEMPTS }}
          AUTH_ATTEMPT_WINDOW=${{ vars.AUTH_ATTEMPT_WINDOW }}
          MAX_CONCURRENT_SESSIONS=${{ vars.MAX_CONCURRENT_SESSIONS }}
          SESSION_INACTIVITY_THRESHOLD=${{ vars.SESSION_INACTIVITY_THRESHOLD }}
          SECURITY_RATE_LIMIT=${{ vars.SECURITY_RATE_LIMIT }}
          SECURITY_RATE_LIMIT_MAX=${{ vars.SECURITY_RATE_LIMIT_MAX }}
          SECURITY_RATE_LIMIT_WINDOW_MS=${{ vars.SECURITY_RATE_LIMIT_WINDOW_MS }}
          TRUST_PROXY=${{ vars.TRUST_PROXY }}
          EMAIL_PROVIDER=${{ vars.EMAIL_PROVIDER }}
          ZEPTOMAIL_ENABLED=${{ vars.ZEPTOMAIL_ENABLED }}
          ZEPTOMAIL_SEND_MAIL_TOKEN=${{ secrets.ZEPTOMAIL_SEND_MAIL_TOKEN }}
          ZEPTOMAIL_FROM_EMAIL=${{ vars.ZEPTOMAIL_FROM_EMAIL }}
          ZEPTOMAIL_FROM_NAME=${{ vars.ZEPTOMAIL_FROM_NAME }}
          ZEPTOMAIL_BOUNCE_ADDRESS=${{ vars.ZEPTOMAIL_BOUNCE_ADDRESS }}
          ZEPTOMAIL_API_BASE_URL=${{ vars.ZEPTOMAIL_API_BASE_URL }}

          # Clinic-Specific Email Configuration (Multi-Tenant)
          # ===================================================
          # ARCHITECTURE: Single Backend API, Multiple Frontends
          # - ONE backend API (API_URL) serves ALL clinics
          # - MULTIPLE frontends (one per clinic) connect to the SAME API
          # - Each frontend sends X-Clinic-ID header to identify which clinic it represents
          # - Only clinic-related credentials differ (email, WhatsApp, SMS, frontend URLs, etc.)
          # - All clinics share the same backend infrastructure (database, cache, services)
          #
          # Pattern: CLINIC_{SANITIZED_CLINIC_NAME}_{CONFIG_KEY}
          # Clinic names are sanitized: spaces/special chars â†’ underscores, uppercase
          # Example: "Vishwamurti Ayurvedelay" â†’ "VISHWAMURTI_AYURVEDELAY"
          #
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example for "Aadesh Ayurvedalay":
          # CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_SEND_MAIL_TOKEN=${{ vars.CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_SEND_MAIL_TOKEN }}
          # CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_FROM_EMAIL=${{ vars.CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_FROM_EMAIL }}
          # CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_FROM_NAME=${{ vars.CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_FROM_NAME }}
          # CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_BOUNCE_ADDRESS=${{ vars.CLINIC_AADESH_AYURVEDELAY_ZEPTOMAIL_BOUNCE_ADDRESS }}
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_SEND_MAIL_TOKEN=${{ secrets.CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_SEND_MAIL_TOKEN }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_FROM_EMAIL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_FROM_EMAIL }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_FROM_NAME=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_FROM_NAME }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_BOUNCE_ADDRESS=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_ZEPTOMAIL_BOUNCE_ADDRESS }}

          # CORS Configuration - Single API, Multiple Frontends
          # =====================================================
          # IMPORTANT: Include ALL clinic frontend URLs in CORS_ORIGIN
          # Format: Comma-separated list (no spaces after commas)
          # Example: "https://ishswami.in,https://www.ishswami.in,https://vishwamurti.viddhakarma.com"
          # All frontends connect to the SAME backend API (API_URL)
          CORS_ORIGIN=${{ vars.CORS_ORIGIN }}
          CORS_CREDENTIALS=${{ vars.CORS_CREDENTIALS }}
          CORS_METHODS=${{ vars.CORS_METHODS }}
          SWAGGER_URL=${{ vars.SWAGGER_URL }}
          BULL_BOARD_URL=${{ vars.BULL_BOARD_URL }}
          SOCKET_URL=${{ vars.SOCKET_URL }}
          PRISMA_STUDIO_URL=${{ vars.PRISMA_STUDIO_URL }}
          PGADMIN_URL=${{ vars.PGADMIN_URL }}
          WHATSAPP_ENABLED=${{ vars.WHATSAPP_ENABLED }}
          WHATSAPP_API_URL=${{ vars.WHATSAPP_API_URL }}
          WHATSAPP_API_KEY=${{ secrets.WHATSAPP_API_KEY }}
          WHATSAPP_PHONE_NUMBER_ID=${{ vars.WHATSAPP_PHONE_NUMBER_ID }}
          WHATSAPP_BUSINESS_ACCOUNT_ID=${{ vars.WHATSAPP_BUSINESS_ACCOUNT_ID }}
          WHATSAPP_OTP_TEMPLATE_ID=${{ vars.WHATSAPP_OTP_TEMPLATE_ID }}
          WHATSAPP_APPOINTMENT_TEMPLATE_ID=${{ vars.WHATSAPP_APPOINTMENT_TEMPLATE_ID }}
          WHATSAPP_PRESCRIPTION_TEMPLATE_ID=${{ vars.WHATSAPP_PRESCRIPTION_TEMPLATE_ID }}

          # Clinic-Specific WhatsApp Configuration (Multi-Tenant)
          # =======================================================
          # Each clinic can have separate WhatsApp credentials
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_API_KEY=${{ secrets.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_API_KEY }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_PHONE_NUMBER_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_PHONE_NUMBER_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_BUSINESS_ACCOUNT_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_BUSINESS_ACCOUNT_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_OTP_TEMPLATE_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_OTP_TEMPLATE_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_APPOINTMENT_TEMPLATE_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_APPOINTMENT_TEMPLATE_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_PRESCRIPTION_TEMPLATE_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_WHATSAPP_PRESCRIPTION_TEMPLATE_ID }}

          # Clinic-Specific Frontend URLs (Multi-Tenant)
          # ============================================
          # ARCHITECTURE: Single Backend API, Multiple Frontends
          # - Each clinic has its own frontend URL (e.g., https://vishwamurti.viddhakarma.com)
          # - All frontends connect to the SAME backend API (API_URL)
          # - Each frontend automatically sends X-Clinic-ID header in all requests
          # - Backend uses X-Clinic-ID to identify which clinic the request is for
          #
          # IMPORTANT: All clinic frontend URLs MUST be added to CORS_ORIGIN variable
          # Format: Comma-separated list (no spaces after commas)
          # Example: "https://ishswami.in,https://vishwamurti.viddhakarma.com,https://clinic2.viddhakarma.com"
          #
          # Pattern: CLINIC_{SANITIZED_CLINIC_NAME}_FRONTEND_URL
          # Example: "Vishwamurti Ayurvedelay" â†’ "VISHWAMURTI_AYURVEDELAY"
          #
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example for "Aadesh Ayurvedalay":
          # CLINIC_AADESH_AYURVEDELAY_FRONTEND_URL=${{ vars.CLINIC_AADESH_AYURVEDELAY_FRONTEND_URL }}
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_FRONTEND_URL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FRONTEND_URL }}

          VIDEO_ENABLED=${{ vars.VIDEO_ENABLED }}
          VIDEO_PROVIDER=${{ vars.VIDEO_PROVIDER }}
          OPENVIDU_URL=${{ vars.OPENVIDU_URL }}
          OPENVIDU_SECRET=${{ secrets.OPENVIDU_SECRET }}
          COTURN_SHARED_SECRET_KEY=${{ secrets.OPENVIDU_SECRET }}
          OPENVIDU_DOMAIN=${{ vars.OPENVIDU_DOMAIN }}
          OPENVIDU_WEBHOOK_ENABLED=${{ vars.OPENVIDU_WEBHOOK_ENABLED }}
          OPENVIDU_WEBHOOK_ENDPOINT=${{ vars.OPENVIDU_WEBHOOK_ENDPOINT }}
          OPENVIDU_WEBHOOK_EVENTS=${{ vars.OPENVIDU_WEBHOOK_EVENTS }}

          # Coturn Configuration (for infrastructure services)
          # These are used by docker-compose.prod.yml for coturn container
          COTURN_PASSWORD=${{ secrets.COTURN_PASSWORD }}
          COTURN_DOMAIN=${{ vars.COTURN_DOMAIN }}
          GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}
          GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}
          GOOGLE_REDIRECT_URI=${{ vars.GOOGLE_REDIRECT_URI }}
          SESSION_SECRET=${{ secrets.SESSION_SECRET }}
          SESSION_TIMEOUT=${{ secrets.SESSION_TIMEOUT }}
          SESSION_SECURE_COOKIES=${{ vars.SESSION_SECURE_COOKIES }}
          SESSION_SAME_SITE=${{ vars.SESSION_SAME_SITE }}
          COOKIE_SECRET=${{ secrets.COOKIE_SECRET }}
          FIREBASE_PROJECT_ID=${{ vars.FIREBASE_PROJECT_ID }}
          FIREBASE_PRIVATE_KEY=${{ secrets.FIREBASE_PRIVATE_KEY }}
          FIREBASE_CLIENT_EMAIL=${{ vars.FIREBASE_CLIENT_EMAIL }}
          FIREBASE_DATABASE_URL=${{ vars.FIREBASE_DATABASE_URL }}
          FIREBASE_VAPID_KEY=${{ secrets.FIREBASE_VAPID_KEY }}

          # Clinic-Specific Firebase Configuration (Multi-Tenant)
          # ======================================================
          # Each clinic can have separate Firebase credentials for push notifications
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_PROJECT_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_PROJECT_ID }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_PRIVATE_KEY=${{ secrets.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_PRIVATE_KEY }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_CLIENT_EMAIL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_CLIENT_EMAIL }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_DATABASE_URL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_DATABASE_URL }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_VAPID_KEY=${{ secrets.CLINIC_VISHWAMURTI_AYURVEDELAY_FIREBASE_VAPID_KEY }}

          # Clinic-Specific SMS Configuration (Multi-Tenant)
          # ================================================
          # Each clinic can have separate SMS provider credentials
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          #
          # Vishwamurti Ayurvedelay (CL0001)
          CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_API_KEY=${{ secrets.CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_API_KEY }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_API_SECRET=${{ secrets.CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_API_SECRET }}
          CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_FROM_NUMBER=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_SMS_FROM_NUMBER }}

          # Clinic-Specific OpenVidu Configuration (Multi-Tenant)
          # ====================================================
          # Each clinic can have separate OpenVidu video server configuration
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_URL=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_URL }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_SECRET }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_DOMAIN=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_OPENVIDU_DOMAIN }}

          # Clinic-Specific S3 Storage Configuration (Multi-Tenant)
          # =======================================================
          # Each clinic can have separate S3 storage bucket and credentials
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_S3_BUCKET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_S3_BUCKET }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_S3_ACCESS_KEY_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_S3_ACCESS_KEY_ID }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_S3_SECRET_ACCESS_KEY=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_S3_SECRET_ACCESS_KEY }}

          # Clinic-Specific Social Auth Configuration (Multi-Tenant)
          # ========================================================
          # Each clinic can have separate OAuth credentials (Google, Facebook, Apple)
          # To add more clinics, duplicate the pattern below with the clinic's sanitized name
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_GOOGLE_CLIENT_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_GOOGLE_CLIENT_ID }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_GOOGLE_CLIENT_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_GOOGLE_CLIENT_SECRET }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_FACEBOOK_APP_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FACEBOOK_APP_ID }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_FACEBOOK_APP_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_FACEBOOK_APP_SECRET }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_APPLE_CLIENT_ID=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_APPLE_CLIENT_ID }}
          # Example: CLINIC_VISHWAMURTI_AYURVEDELAY_APPLE_CLIENT_SECRET=${{ vars.CLINIC_VISHWAMURTI_AYURVEDELAY_APPLE_CLIENT_SECRET }}

          FACEBOOK_APP_ID=${{ vars.FACEBOOK_APP_ID }}
          FACEBOOK_APP_SECRET=${{ vars.FACEBOOK_APP_SECRET }}
          APPLE_CLIENT_ID=${{ vars.APPLE_CLIENT_ID }}
          APPLE_CLIENT_SECRET=${{ vars.APPLE_CLIENT_SECRET }}
          S3_ENABLED=${{ vars.S3_ENABLED }}
          S3_PROVIDER=${{ vars.S3_PROVIDER }}
          S3_ENDPOINT=${{ vars.S3_ENDPOINT }}
          S3_REGION=${{ vars.S3_REGION }}
          S3_BUCKET=${{ vars.S3_BUCKET }}
          S3_ACCESS_KEY_ID=${{ secrets.S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY=${{ secrets.S3_SECRET_ACCESS_KEY }}
          S3_FORCE_PATH_STYLE=${{ vars.S3_FORCE_PATH_STYLE }}
          S3_PUBLIC_URL_EXPIRATION=${{ vars.S3_PUBLIC_URL_EXPIRATION }}
          CDN_URL=${{ vars.CDN_URL }}
          DOCKER_ENV=${{ vars.DOCKER_ENV }}
          DOCKER_NETWORK=${{ vars.DOCKER_NETWORK }}

          # Optional Service URLs
          REDIS_COMMANDER_URL=${{ vars.REDIS_COMMANDER_URL }}

          # Jitsi Configuration (Fallback Video Provider - Optional)
          JITSI_DOMAIN=${{ vars.JITSI_DOMAIN }}
          JITSI_BASE_DOMAIN=${{ vars.JITSI_BASE_DOMAIN }}
          JITSI_SUBDOMAIN=${{ vars.JITSI_SUBDOMAIN }}
          JITSI_APP_ID=${{ vars.JITSI_APP_ID }}
          JITSI_APP_SECRET=${{ secrets.JITSI_APP_SECRET }}
          JITSI_BASE_URL=${{ vars.JITSI_BASE_URL }}
          JITSI_WS_URL=${{ vars.JITSI_WS_URL }}
          JITSI_ENABLE_RECORDING=${{ vars.JITSI_ENABLE_RECORDING }}
          JITSI_ENABLE_WAITING_ROOM=${{ vars.JITSI_ENABLE_WAITING_ROOM }}

          # Social Auth Redirect URIs
          FACEBOOK_REDIRECT_URI=${{ vars.FACEBOOK_REDIRECT_URI }}
          APPLE_REDIRECT_URI=${{ vars.APPLE_REDIRECT_URI }}
          EOF

      - name: Copy files to server
        run: |
          # Copy all deployment scripts
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts -r devops/scripts/shared devops/scripts/docker-infra ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/opt/healthcare-backend/devops/scripts/
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts /tmp/.env.production ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/.env.production
          # Copy docker-compose file
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/docker/docker-compose.prod.yml ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/docker-compose.prod.yml

      - name: Set deployment image name
        id: deploy-image
        run: |
          REPO_LOWER=$(echo "${{ env.IMAGE_REPO }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_FULL="ghcr.io/${REPO_LOWER}/${{ env.IMAGE_NAME }}"
          echo "image=${IMAGE_FULL}" >> $GITHUB_OUTPUT
          echo "Image: ${IMAGE_FULL}"

      - name: Validate app image (non-blocking)
        continue-on-error: true
        run: |
          echo "ðŸ” Validating app image in registry..."

          # Try to validate latest tag first (most reliable for main branch)
          APP_IMAGE_LATEST="${{ steps.deploy-image.outputs.image }}:latest"
          APP_IMAGE_SHA="${{ steps.deploy-image.outputs.image }}:main-${{ github.sha }}"

          # Check latest tag first
          if docker manifest inspect "${APP_IMAGE_LATEST}" > /dev/null 2>&1; then
            echo "âœ… App image validated (latest tag): ${APP_IMAGE_LATEST}"
          # Fallback to SHA-based tag
          elif docker manifest inspect "${APP_IMAGE_SHA}" > /dev/null 2>&1; then
            echo "âœ… App image validated (SHA tag): ${APP_IMAGE_SHA}"
          else
            echo "âš ï¸  App image not immediately available in registry (may be propagating)"
            echo "   This is non-blocking - Docker will validate during pull"
            echo "   Attempted tags:"
            echo "     - ${APP_IMAGE_LATEST}"
            echo "     - ${APP_IMAGE_SHA}"
          fi

          # Note: Infrastructure images are pulled from docker-compose during deployment
          # They are validated when docker-compose pulls them
          echo "â„¹ï¸  Infrastructure images will be validated during deployment"

      - name: Setup server directories
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            set -e
            
            # Run directory setup script
            if [[ -f /opt/healthcare-backend/devops/scripts/docker-infra/setup-directories.sh ]]; then
              echo "Setting up server directories..."
              chmod +x /opt/healthcare-backend/devops/scripts/docker-infra/setup-directories.sh
              /opt/healthcare-backend/devops/scripts/docker-infra/setup-directories.sh
            else
              echo "Setup script not found, creating directories manually..."
              mkdir -p /opt/healthcare-backend/backups/{postgres,dragonfly,metadata}
              mkdir -p /opt/healthcare-backend/data/{postgres,dragonfly,openvidu_recordings}
              # /var/log/deployments requires sudo if not running as root
              if [[ "$EUID" -eq 0 ]]; then
                mkdir -p /var/log/deployments
                chmod 755 /var/log/deployments
              else
                sudo mkdir -p /var/log/deployments
                sudo chmod 755 /var/log/deployments
              fi
              chmod 700 /opt/healthcare-backend/backups
              chmod 755 /opt/healthcare-backend/data
              echo "Directories created"
            fi
          ENDSSH

      - name: Deploy to server via SSH
        env:
          SERVER_DEPLOY_PATH: ${{ secrets.SERVER_DEPLOY_PATH }}
          IMAGE_TAG: main-${{ github.sha }}
          REGISTRY: ${{ env.REGISTRY }}
          IMAGE: ${{ steps.deploy-image.outputs.image }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_USERNAME: ${{ github.actor }}
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            set -e
            
            # Set deployment variables (values embedded from GitHub Actions)
            # Handle SERVER_DEPLOY_PATH with default value and input validation
            # Security: Validate SERVER_DEPLOY_PATH to prevent path traversal attacks
            SERVER_DEPLOY_PATH_SECRET_VALUE="${{ secrets.SERVER_DEPLOY_PATH }}"
            
            # Input validation: Only allow alphanumeric, forward slashes, hyphens, and underscores
            # Prevent path traversal and command injection
            if [ -n "${SERVER_DEPLOY_PATH_SECRET_VALUE}" ] && [ "${SERVER_DEPLOY_PATH_SECRET_VALUE}" != "" ]; then
              # Validate path format (no .., no spaces, no special chars except /, -, _)
              if [[ ! "${SERVER_DEPLOY_PATH_SECRET_VALUE}" =~ ^[a-zA-Z0-9/_-]+$ ]] || [[ "${SERVER_DEPLOY_PATH_SECRET_VALUE}" == *".."* ]]; then
                echo "âŒ ERROR: Invalid SERVER_DEPLOY_PATH format (security check failed)"
                exit 1
              fi
              export SERVER_DEPLOY_PATH="${SERVER_DEPLOY_PATH_SECRET_VALUE}"
              echo "âœ… Using SERVER_DEPLOY_PATH from secret: '${SERVER_DEPLOY_PATH}'"
            else
              export SERVER_DEPLOY_PATH="/opt/healthcare-backend"
              echo "âœ… Using default SERVER_DEPLOY_PATH: '${SERVER_DEPLOY_PATH}'"
            fi
            
            # Final safety check - should never be empty at this point
            if [ -z "${SERVER_DEPLOY_PATH}" ] || [ "${SERVER_DEPLOY_PATH}" = "" ]; then
              echo "âŒ ERROR: SERVER_DEPLOY_PATH is empty! Forcing to default..."
              export SERVER_DEPLOY_PATH="/opt/healthcare-backend"
            fi
            
            # One final verification
            if [ -z "${SERVER_DEPLOY_PATH}" ]; then
              echo "âŒ FATAL: SERVER_DEPLOY_PATH is still empty after all attempts!"
              exit 1
            fi
            
            # Security: Don't log full path in production logs (masked by GitHub Actions)
            echo "ðŸ“ Final SERVER_DEPLOY_PATH='[REDACTED]'"
            export IMAGE_TAG="main-${{ github.sha }}"
            export REGISTRY="${{ env.REGISTRY }}"
            export IMAGE="${{ steps.deploy-image.outputs.image }}"
            export GITHUB_TOKEN="${{ secrets.GITHUB_TOKEN }}"
            export GITHUB_USERNAME="${{ github.actor }}"
            
            # Debug: Show deployment info (secrets masked by GitHub Actions)
            echo "Deploying image: ${IMAGE}:${IMAGE_TAG}"
            # Security: Don't log full path in production logs
            echo "Server deployment path: [REDACTED]"
            
            # Validate IMAGE is set
            if [ -z "${IMAGE}" ] || [ "${IMAGE}" = "ghcr.io/your-username/your-repo/healthcare-api" ]; then
              echo "ERROR: IMAGE environment variable is not set or invalid"
              echo "IMAGE value: '${IMAGE}'"
              exit 1
            fi
            
            # Validate SERVER_DEPLOY_PATH is set (should never be empty after our check above)
            if [ -z "${SERVER_DEPLOY_PATH}" ]; then
              echo "âŒ ERROR: SERVER_DEPLOY_PATH is empty after default assignment!"
              echo "This should never happen. Check the logic above."
              exit 1
            fi
            
            # Create deployment directory if it doesn't exist (deploy.sh expects this)
            # Use SERVER_DEPLOY_PATH directly - no intermediate variable needed
            # Verify it's still set before using it
            if [ -z "${SERVER_DEPLOY_PATH}" ]; then
              echo "âŒ ERROR: SERVER_DEPLOY_PATH is empty when trying to create directory!"
              exit 1
            fi
            
            echo "ðŸ“ Creating deployment directory: ${SERVER_DEPLOY_PATH}"
            mkdir -p "${SERVER_DEPLOY_PATH}/devops/docker" || {
              echo "âŒ ERROR: Failed to create directory: ${SERVER_DEPLOY_PATH}/devops/docker"
              exit 1
            }
            
            echo "ðŸ“„ Moving .env.production to ${SERVER_DEPLOY_PATH}/.env.production"
            mv /tmp/.env.production "${SERVER_DEPLOY_PATH}/.env.production" || {
              echo "âŒ ERROR: Failed to move .env.production to ${SERVER_DEPLOY_PATH}/.env.production"
              exit 1
            }
            chmod 600 "${SERVER_DEPLOY_PATH}/.env.production"
            
            echo "ðŸ“„ Moving docker-compose.prod.yml to ${SERVER_DEPLOY_PATH}/devops/docker/docker-compose.prod.yml"
            mv /tmp/docker-compose.prod.yml "${SERVER_DEPLOY_PATH}/devops/docker/docker-compose.prod.yml" || {
              echo "âŒ ERROR: Failed to move docker-compose.prod.yml"
              exit 1
            }
            
            # Make scripts executable
            chmod +x /opt/healthcare-backend/devops/scripts/docker-infra/*.sh
            chmod +x /opt/healthcare-backend/devops/scripts/shared/*.sh
            
            # VALIDATION: Check OPENVIDU_SECRET format before deployment
            # OpenVidu secret must contain only alphanumeric [a-zA-Z0-9], hyphens (-), underscores (_)
            OPENVIDU_SECRET="${OPENVIDU_SECRET:-}"
            if [[ -n "${OPENVIDU_SECRET}" ]] && echo "${OPENVIDU_SECRET}" | grep -q '[^a-zA-Z0-9_-]'; then
              echo "âŒ ERROR: OPENVIDU_SECRET contains invalid characters!"
              echo "ðŸ“‹ OpenVidu requires secret to contain ONLY:"
              echo "   - Alphanumeric: a-z, A-Z, 0-9"
              echo "   - Hyphens: - | Underscores: _"
              echo "âŒ NOT allowed: = + / or any other special characters"
              echo ""
              echo "ðŸ”§ ACTION: Update OPENVIDU_SECRET in GitHub Secrets"
              exit 1
            fi
            
            # Set deployment variables for deploy.sh
            export INFRA_CHANGED="${{ needs.detect-changes.outputs.infra-changed }}"
            export APP_CHANGED="${{ needs.detect-changes.outputs.app-changed }}"
            export INFRA_HEALTHY="${{ needs.check-infrastructure.outputs.infra-healthy }}"
            export INFRA_STATUS="${{ needs.check-infrastructure.outputs.infra-status }}"
            export BACKUP_ID="${{ needs.backup-infrastructure.outputs.backup-id || '' }}"
            
            # Set DOCKER_IMAGE for docker-compose (uses IMAGE:IMAGE_TAG format)
            export DOCKER_IMAGE="${IMAGE}:${IMAGE_TAG}"
            echo "Setting DOCKER_IMAGE=${DOCKER_IMAGE}"
            
            # Flag to indicate infrastructure operations were already handled by CI/CD jobs
            # This prevents deploy.sh from duplicating backup/recreate/restore operations
            if [[ "${{ needs.detect-changes.outputs.infra-changed }}" == "true" ]] || \
               [[ "${{ needs.check-infrastructure.outputs.infra-healthy }}" == "false" ]]; then
              export INFRA_ALREADY_HANDLED="true"
            else
              export INFRA_ALREADY_HANDLED="false"
            fi
            
            # Run smart deployment orchestrator
            # Note: deploy.sh will skip infrastructure operations if INFRA_ALREADY_HANDLED=true
            # and only handle application deployment
            cd /opt/healthcare-backend
            /opt/healthcare-backend/devops/scripts/docker-infra/deploy.sh
            
          ENDSSH

      - name: Deployment Success
        run: |
          echo "âœ… Deployment completed successfully!"
          echo "Image: ${{ steps.deploy-image.outputs.image }}:main-${{ github.sha }}"
          # Security: Don't log server hostname in public logs
          echo "Server: [REDACTED]"

  # Post-Deployment Verification & Backup
  # Goal: No infra issues, no API issues. Verifies infra + API health (verify.sh deployment); fails job if any issue.
  # Then creates a success backup as a recovery point.
  post-deployment-verification:
    name: Post-Deployment Verification & Backup
    runs-on: ubuntu-latest
    if: |
      always() &&
      github.event_name == 'push' && 
      github.ref == 'refs/heads/main' &&
      needs.deploy.result == 'success'
    environment: production
    needs: [deploy]
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4

      - name: Configure SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add server to known hosts
        run: |
          mkdir -p ~/.ssh
          chmod 700 ~/.ssh
          ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy verification and backup scripts
        run: |
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/verify.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/verify.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/docker-infra/backup.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/backup.sh
          scp -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts devops/scripts/shared/utils.sh ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }}:/tmp/utils.sh

      - name: Verify deployment health (infra + API; fail if any issue)
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            chmod +x /tmp/verify.sh
            
            echo "ðŸ” Verifying deployment: infra + API health (goal: no infra issues, no API issues)..."
            if /tmp/verify.sh deployment; then
              echo "âœ… Deployment verification passed (infra and API healthy)"
              echo "VERIFICATION_STATUS=success" > /tmp/post-deploy-status.txt
            else
              echo "âŒ Deployment verification failed (infra or API has issues)"
              echo "VERIFICATION_STATUS=failure" > /tmp/post-deploy-status.txt
              exit 1
            fi
          ENDSSH

      - name: Create success backup
        env:
          S3_ENABLED: ${{ vars.S3_ENABLED }}
          S3_PROVIDER: ${{ vars.S3_PROVIDER }}
          S3_ENDPOINT: ${{ vars.S3_ENDPOINT }}
          S3_REGION: ${{ vars.S3_REGION }}
          S3_BUCKET: ${{ vars.S3_BUCKET }}
          S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
          S3_FORCE_PATH_STYLE: ${{ vars.S3_FORCE_PATH_STYLE }}
        run: |
          ssh -o StrictHostKeyChecking=accept-new -o UserKnownHostsFile=~/.ssh/known_hosts ${{ secrets.SERVER_USER }}@${{ secrets.SERVER_HOST }} << 'ENDSSH'
            set -e
            export SCRIPT_DIR="/tmp"
            source /tmp/utils.sh
            export S3_ENABLED="${{ vars.S3_ENABLED }}"
            export S3_PROVIDER="${{ vars.S3_PROVIDER }}"
            export S3_ENDPOINT="${{ vars.S3_ENDPOINT }}"
            export S3_REGION="${{ vars.S3_REGION }}"
            export S3_BUCKET="${{ vars.S3_BUCKET }}"
            export S3_ACCESS_KEY_ID="${{ secrets.S3_ACCESS_KEY_ID }}"
            export S3_SECRET_ACCESS_KEY="${{ secrets.S3_SECRET_ACCESS_KEY }}"
            export S3_FORCE_PATH_STYLE="${{ vars.S3_FORCE_PATH_STYLE }}"
            
            # Install rclone or s3cmd if needed
            if ! command -v rclone &> /dev/null && ! command -v s3cmd &> /dev/null; then
              echo "ðŸ“¦ Installing rclone for Contabo S3..."
              if curl -fsSL https://rclone.org/install.sh | sudo bash; then
                echo "âœ… rclone installed successfully"
              else
                echo "âš ï¸  rclone installation failed, trying s3cmd fallback..."
                if command -v apt-get &> /dev/null; then
                  sudo apt-get update -qq
                  sudo apt-get install -y s3cmd || {
                    if command -v pip3 &> /dev/null; then
                      pip3 install --user s3cmd && export PATH="$HOME/.local/bin:$PATH" || {
                        echo "âŒ Failed to install both rclone and s3cmd"
                        exit 1
                      }
                    else
                      echo "âŒ Cannot install rclone or s3cmd automatically"
                      exit 1
                    fi
                  }
                elif command -v pip3 &> /dev/null; then
                  pip3 install --user s3cmd && export PATH="$HOME/.local/bin:$PATH" || {
                    echo "âŒ Failed to install both rclone and s3cmd"
                    exit 1
                  }
                else
                  echo "âŒ Cannot install rclone or s3cmd automatically"
                  exit 1
                fi
                echo "âœ… s3cmd installed as fallback"
              fi
            elif command -v rclone &> /dev/null; then
              echo "âœ… rclone is already installed"
            elif command -v s3cmd &> /dev/null; then
              echo "âœ… s3cmd is already installed (rclone preferred but s3cmd will work)"
            fi
            
            chmod +x /tmp/backup.sh
            
            echo "ðŸ’¾ Creating success backup after deployment..."
            echo "ðŸ“‹ This backup serves as a recovery point for the successful deployment"
            BACKUP_ID=$(/tmp/backup.sh success) || {
              echo "âš ï¸  Success backup failed (non-critical, but recommended)"
              exit 0  # Don't fail the job if backup fails
            }
            echo "âœ… Success backup created (ID: ${BACKUP_ID})"
            echo "BACKUP_ID=${BACKUP_ID}" > /tmp/post-deploy-backup-id.txt
          ENDSSH

      - name: Post-deployment summary
        run: |
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ðŸ“Š Post-Deployment Summary"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "âœ… Deployment verification completed"
          echo "ðŸ’¾ Success backup created (if backup succeeded)"
          echo "ðŸŽ‰ Deployment is healthy and backed up!"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
