# Quick Start Development Environment
# Run with: docker-compose -f devops/docker/docker-compose.dev.yml up -d
# Access: http://localhost:8088
#
# Environment Variables:
# - Loads .env (base configuration)
# - Loads .env.development (development overrides)
# - .env.local has highest priority (if exists, not in git)
# - Variables can be overridden via environment or docker-compose environment section

services:
  api:
    build:
      context: ../..
      dockerfile: devops/docker/Dockerfile.dev
    container_name: healthcare-api
    # Load environment variables from .env files (priority: .env.local > .env.development > .env)
    env_file:
      - ../../.env
      - ../../.env.development
      - ../../.env.local  # Highest priority, optional
    ports:
      - "8088:8088"
      - "5555:5555"
    environment:
      # Application Environment
      NODE_ENV: development
      PORT: 8088
      HOST: 0.0.0.0
      BIND_ADDRESS: 0.0.0.0
      DOCKER_ENV: "true"
      SERVICE_NAME: api
      
      # Database Configuration
      # Connection pool parameters: connection_limit=50, pool_timeout=20
      # Increased for 10M+ users scalability and to prevent connection pool exhaustion
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/userdb?connection_limit=50&pool_timeout=20
      # DIRECT_URL: Clean connection string for Prisma Studio (without Prisma-specific parameters)
      # Prisma Studio needs this to connect properly without "unrecognized configuration parameter" errors
      DIRECT_URL: postgresql://postgres:postgres@postgres:5432/userdb
      PRISMA_SCHEMA_PATH: /app/src/libs/infrastructure/database/prisma/schema.prisma
      
      # Cache Provider Configuration (Dragonfly is default)
      CACHE_ENABLED: "true"
      CACHE_PROVIDER: dragonfly
      
      # Redis Configuration (for Redis provider) - Commented out, using Dragonfly
      # REDIS_HOST: redis
      # REDIS_PORT: 6379
      # REDIS_DB: 0
      # REDIS_TTL: 3600
      # REDIS_PREFIX: "healthcare:"
      # REDIS_ENABLED: "true"
      
      # Dragonfly Configuration (Primary Cache Provider - default provider - using instead of Redis)
      DRAGONFLY_ENABLED: "true"
      DRAGONFLY_HOST: dragonfly
      DRAGONFLY_PORT: 6379
      DRAGONFLY_KEY_PREFIX: "healthcare:"
      
      # JWT Configuration
      JWT_SECRET: dev-jwt-secret-key-change-in-production
      JWT_EXPIRATION: 24h
      JWT_ACCESS_EXPIRES_IN: 24h
      JWT_REFRESH_EXPIRES_IN: 7d
      
      # Session Configuration (Fastify Session with CacheService/Dragonfly)
      SESSION_SECRET: dev-session-secret-change-in-production-min-32-chars-long
      SESSION_TIMEOUT: 86400
      SESSION_SECURE_COOKIES: "false"
      SESSION_SAME_SITE: "lax"
      COOKIE_SECRET: dev-cookie-secret-change-in-production-min-32-chars
      
      # Base URLs
      BASE_URL: http://localhost:8088
      API_URL: http://localhost:8088
      FRONTEND_URL: http://localhost:3000
      
      # CORS Configuration
      CORS_ORIGIN: http://localhost:3000,http://localhost:8088,http://localhost:5050,http://localhost:8082,https://localhost:4443,https://localhost:8443
      CORS_CREDENTIALS: "true"
      
      # Service URLs
      SWAGGER_URL: /docs
      BULL_BOARD_URL: /queue-dashboard
      SOCKET_URL: /socket.io
      REDIS_COMMANDER_URL: http://localhost:8082
      PRISMA_STUDIO_URL: http://localhost:5555
      PGADMIN_URL: http://localhost:5050
      
      # Video Configuration (Dual Provider: OpenVidu primary, Jitsi fallback)
      VIDEO_ENABLED: ${VIDEO_ENABLED:-true}
      VIDEO_PROVIDER: ${VIDEO_PROVIDER:-openvidu}
      
      # OpenVidu Configuration (Primary Video Provider - from .env files)
      # Values are loaded from .env.development via env_file directive above
      OPENVIDU_ENABLED: ${VIDEO_ENABLED:-true}
      OPENVIDU_URL: ${OPENVIDU_URL:-https://localhost:4443}
      OPENVIDU_SECRET: ${OPENVIDU_SECRET:-MY_SECRET}
      OPENVIDU_DOMAIN: ${OPENVIDU_DOMAIN:-localhost}
      # OpenVidu Webhook Configuration (Optimized Architecture: Webhooks → Backend → Socket.IO)
      # Set to true to enable webhooks and reduce Socket.IO load for video events
      OPENVIDU_WEBHOOK_ENABLED: ${OPENVIDU_WEBHOOK_ENABLED:-false}
      OPENVIDU_WEBHOOK_ENDPOINT: ${OPENVIDU_WEBHOOK_ENDPOINT:-http://api:8088/api/v1/webhooks/openvidu}
      OPENVIDU_WEBHOOK_EVENTS: ${OPENVIDU_WEBHOOK_EVENTS:-sessionCreated,sessionDestroyed,participantJoined,participantLeft,recordingStarted,recordingStopped}
      
      # Jitsi Configuration (Fallback - like Redis in cache pattern)
      JITSI_DOMAIN: ${JITSI_DOMAIN:-localhost:8443}
      JITSI_BASE_DOMAIN: ${JITSI_BASE_DOMAIN:-localhost}
      JITSI_SUBDOMAIN: ${JITSI_SUBDOMAIN:-localhost}
      JITSI_APP_ID: ${JITSI_APP_ID:-healthcare-jitsi-app}
      JITSI_APP_SECRET: ${JITSI_APP_SECRET:-dev-jitsi-secret-change-in-production}
      JITSI_BASE_URL: ${JITSI_BASE_URL:-https://localhost:8443}
      JITSI_WS_URL: ${JITSI_WS_URL:-wss://localhost:8443/xmpp-websocket}
      JITSI_ENABLE_RECORDING: ${JITSI_ENABLE_RECORDING:-true}
      JITSI_ENABLE_WAITING_ROOM: ${JITSI_ENABLE_WAITING_ROOM:-true}
      
      # Google OAuth Configuration
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID:-}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET:-}
      GOOGLE_REDIRECT_URI: http://localhost:8088/auth/google/callback
      
      # API Configuration
      API_PREFIX: /api/v1
      
      # Logging Configuration
      LOG_LEVEL: debug
      ENABLE_AUDIT_LOGS: "true"
      
      # Security Configuration
      SECURITY_RATE_LIMIT: "true"
      SECURITY_RATE_LIMIT_MAX: 1000
      SECURITY_RATE_LIMIT_WINDOW_MS: 15000
      TRUST_PROXY: 1
      
      # Rate Limiting
      RATE_LIMIT_TTL: 60
      RATE_LIMIT_MAX: 100
      
      # Email Configuration (Optional - using defaults)
      EMAIL_HOST: sandbox.smtp.mailtrap.io
      EMAIL_PORT: 2525
      EMAIL_SECURE: "false"
      EMAIL_FROM: noreply@healthcare.com
      
      # Development Tools
      ENABLE_SWAGGER: "true"
      ENABLE_PRISMA_STUDIO: "true"
    volumes:
      - ../../:/app
      - healthcare_node_modules:/app/node_modules
      - healthcare_pnpm_store:/pnpm-store
      - ./logs:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      dragonfly:
        condition: service_healthy
      # redis:
      #   condition: service_healthy
    networks:
      - healthcare-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8088/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    command: >
      sh -c "
        if [ ! -f /app/node_modules/.bin/cross-env ]; then
          echo 'Installing dependencies (pnpm)...' &&
          pnpm install --frozen-lockfile --ignore-scripts --store-dir /pnpm-store;
        else
          echo 'Dependencies already installed, skipping pnpm install';
        fi &&
        echo 'Waiting for database to be ready...' &&
        until pg_isready -h postgres -p 5432 -U postgres; do
          echo 'Waiting for postgres...'
          sleep 2
        done &&
        echo 'Database is ready!' &&
        echo 'Generating Prisma Client...' &&
        (pnpm prisma generate --schema=./src/libs/infrastructure/database/prisma/schema.prisma || echo 'Prisma generate completed') &&
        (node scripts/fix-prisma-sourcemaps.js || echo 'Prisma sourcemap fix completed') &&
        echo 'Fixing Prisma types...' &&
        (pnpm exec ts-node -r tsconfig-paths/register scripts/fix-prisma-types.ts || echo 'Fix script completed') &&
        echo 'Running Prisma Database Sync (Development Mode)...' &&
        echo 'Using db push for development (no migration history required)...' &&
        pnpm prisma:db:push || echo 'Database push completed or skipped' &&
        echo 'Starting application...' &&
        pnpm start:dev
      "

  postgres:
    image: postgres:16-alpine
    container_name: healthcare-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: userdb
    command: >
      postgres
      -c max_connections=100
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=4
      -c max_parallel_workers_per_gather=2
      -c max_parallel_workers=4
      -c max_parallel_maintenance_workers=2
      -c idle_in_transaction_session_timeout=30000
      -c statement_timeout=30000
      -c lock_timeout=10000
      -c listen_addresses='*'
      -c log_connections=on
      -c log_disconnections=on
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - healthcare-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  dragonfly:
    # Using official Dragonfly registry with specific version (more reliable than latest)
    # If network timeout persists, try: docker pull docker.dragonflydb.io/dragonflydb/dragonfly:v1.27.1 manually first
    image: docker.dragonflydb.io/dragonflydb/dragonfly:v1.27.1
    container_name: healthcare-dragonfly
    ports:
      - "6380:6379"
    command: >
      --alsologtostderr
      --cache_mode=false
      --maxmemory=2gb
      --proactor_threads=4
      --default_lua_flags=allow-undeclared-keys
    volumes:
      - dragonfly_data:/data
    networks:
      - healthcare-network
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -p 6379 ping || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M

  # Redis - Commented out, using Dragonfly instead (primary cache provider)
  # Uncomment only if you need Redis as a fallback cache provider
  # redis:
  #   image: redis:7-alpine
  #   container_name: healthcare-redis
  #   ports:
  #     - "6379:6379"
  #   command: redis-server --appendonly yes
  #   volumes:
  #     - redis_data:/data
  #   networks:
  #     - healthcare-network
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 5s
  #     timeout: 5s
  #     retries: 5
  #   restart: unless-stopped

  # Redis Commander - Optional: Only needed if using Redis (commented out by default)
  # Uncomment only if you need Redis UI for Redis (not needed for Dragonfly)
  # redis-commander:
  #   image: rediscommander/redis-commander:latest
  #   container_name: healthcare-redis-ui
  #   ports:
  #     - "8082:8081"
  #   environment:
  #     - REDIS_HOSTS=dragonfly:dragonfly:6379
  #     - HTTP_USER=admin
  #     - HTTP_PASSWORD=admin
  #   depends_on:
  #     dragonfly:
  #       condition: service_healthy
  #     # redis:
  #     #   condition: service_healthy
  #   networks:
  #     - healthcare-network
  #   restart: unless-stopped

  worker:
    build:
      context: ../..
      dockerfile: devops/docker/Dockerfile.dev
    container_name: healthcare-worker
    # Load environment variables from .env files (priority: .env.local > .env.development > .env)
    env_file:
      - ../../.env
      - ../../.env.development
      - ../../.env.local  # Highest priority, optional
    environment:
      # Application Environment
      NODE_ENV: development
      APP_MODE: worker
      SERVICE_NAME: worker
      DOCKER_ENV: "true"
      
      # Database Configuration
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/userdb?connection_limit=50&pool_timeout=20
      PRISMA_SCHEMA_PATH: /app/src/libs/infrastructure/database/prisma/schema.prisma
      
      # Cache Provider Configuration (Dragonfly is default)
      CACHE_ENABLED: "true"
      CACHE_PROVIDER: dragonfly
      DRAGONFLY_ENABLED: "true"
      DRAGONFLY_HOST: dragonfly
      DRAGONFLY_PORT: 6379
      DRAGONFLY_KEY_PREFIX: "healthcare:"
      
      # JWT Configuration (for queue operations)
      JWT_SECRET: dev-jwt-secret-key-change-in-production
      
      # BullMQ Worker Configuration
      BULL_WORKER_CONCURRENCY: "5"
      BULL_MAX_JOBS_PER_WORKER: "100"
      
      # Logging Configuration
      LOG_LEVEL: debug
      ENABLE_AUDIT_LOGS: "true"
    volumes:
      - ../../:/app
      - healthcare_node_modules:/app/node_modules
      - healthcare_pnpm_store:/pnpm-store
      - ./logs:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      dragonfly:
        condition: service_healthy
      api:
        condition: service_started
    networks:
      - healthcare-network
    restart: unless-stopped
    command: >
      sh -c "
        if [ ! -f /app/node_modules/.bin/ts-node ]; then
          echo 'Installing dependencies (pnpm)...' &&
          pnpm install --frozen-lockfile --ignore-scripts --store-dir /pnpm-store;
        else
          echo 'Dependencies already installed, skipping pnpm install';
        fi &&
        echo 'Waiting for database to be ready...' &&
        until pg_isready -h postgres -p 5432 -U postgres; do
          echo 'Waiting for postgres...'
          sleep 2
        done &&
        echo 'Database is ready!' &&
        echo 'Generating Prisma Client...' &&
        (pnpm prisma generate --schema=./src/libs/infrastructure/database/prisma/schema.prisma || echo 'Prisma generate completed') &&
        (node scripts/fix-prisma-sourcemaps.js || echo 'Prisma sourcemap fix completed') &&
        echo 'Starting worker process...' &&
        pnpm exec ts-node -r tsconfig-paths/register src/worker-bootstrap.ts
      "

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: healthcare-pgadmin
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION: "True"
      # PGADMIN_CONFIG_LOGIN_BANNER: "Welcome to Healthcare Database Management"  # Commented out due to syntax error
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - healthcare-network
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://localhost/misc/ping').read()\" || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 40s
    restart: unless-stopped

  # OpenVidu Services
  openvidu-server:
    image: openvidu/openvidu-server-kms:latest
    container_name: healthcare-openvidu-server
    # Load environment variables from .env files (priority: .env.local > .env.development > .env)
    env_file:
      - ../../.env
      - ../../.env.development
      - ../../.env.local  # Highest priority, optional
    ports:
      - "4443:4443"
      # NOTE: Port 3478 is NOT needed here - OpenVidu uses coturn service for TURN/STUN
      # coturn container handles port 3478, OpenVidu connects to it via COTURN_IP=coturn
      # Reduced port range for Windows compatibility (OpenVidu uses coturn for TURN/STUN)
      # Smaller range (100 ports) avoids Windows permission issues while still allowing media streams
      - "50000-50100:50000-50100/udp"
      - "50000-50100:50000-50100/tcp"
    environment:
      # OpenVidu Configuration (from .env files, with fallbacks)
      - OPENVIDU_SECRET=${OPENVIDU_SECRET:-MY_SECRET}
      - OPENVIDU_PUBLICURL=${OPENVIDU_URL:-https://localhost:4443}
      - OPENVIDU_DOMAIN=${OPENVIDU_DOMAIN:-localhost}
      - OPENVIDU_EDITION=ce
      - OPENVIDU_CDR=false
      # OpenVidu Webhook Configuration (from .env files)
      - OPENVIDU_WEBHOOK=${OPENVIDU_WEBHOOK_ENABLED:-false}
      - OPENVIDU_WEBHOOK_ENDPOINT=${OPENVIDU_WEBHOOK_ENDPOINT:-http://api:8088/api/v1/webhooks/openvidu}
      - OPENVIDU_WEBHOOK_EVENTS=${OPENVIDU_WEBHOOK_EVENTS:-sessionCreated,sessionDestroyed,participantJoined,participantLeft,recordingStarted,recordingStopped}
      - OPENVIDU_WEBHOOK_HEADERS=
      - OPENVIDU_WEBHOOK_EVENTS_TO_IGNORE=
      # OpenVidu Stream Configuration
      - OPENVIDU_STREAMS_VIDEO_MAX_RECV_BANDWIDTH=1000
      - OPENVIDU_STREAMS_VIDEO_MIN_RECV_BANDWIDTH=300
      - OPENVIDU_STREAMS_VIDEO_MAX_SEND_BANDWIDTH=1000
      - OPENVIDU_STREAMS_VIDEO_MIN_SEND_BANDWIDTH=300
      - OPENVIDU_STREAMS_FORCED_VIDEO_CODEC=VP8
      - OPENVIDU_STREAMS_ALLOW_TRANSCODING=false
      # OpenVidu Session Configuration
      - OPENVIDU_SESSIONS_GARBAGE_INTERVAL=900
      - OPENVIDU_SESSIONS_GARBAGE_THRESHOLD=3600
      # OpenVidu Recording Configuration
      - OPENVIDU_RECORDING=true
      - OPENVIDU_RECORDING_PATH=/opt/openvidu/recordings
      - OPENVIDU_RECORDING_PUBLIC_ACCESS=false
      - OPENVIDU_RECORDING_NOTIFICATION=publisher_moderator
      - OPENVIDU_RECORDING_CUSTOM_LAYOUT=/opt/openvidu/custom-layout
      - OPENVIDU_RECORDING_AUTOSTOP_TIMEOUT=120
      - OPENVIDU_RECORDING_ALWAYS=never
      # Coturn Configuration
      - COTURN_IP=coturn
      # Performance optimizations for faster startup
      - OPENVIDU_LOGS_LEVEL=WARN
      # Required: Set domain for proper configuration (prevents NullPointerException)
      - DOMAIN_OR_PUBLIC_IP=${OPENVIDU_DOMAIN:-localhost}
      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL:-}
      - CERTIFICATE_TYPE=${CERTIFICATE_TYPE:-selfsigned}
      - TZ=UTC
    volumes:
      - openvidu_recordings:/opt/openvidu/recordings
      - openvidu_custom_layouts:/opt/openvidu/custom-layout
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      coturn:
        condition: service_healthy
    networks:
      - healthcare-network
    restart: unless-stopped
    healthcheck:
      # Simplified healthcheck - just check if supervisor is running
      # OpenVidu can take 60-120 seconds to fully start, so we give it more time
      test: ["CMD-SHELL", "supervisorctl status | grep -q RUNNING || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 120s

  coturn:
    image: coturn/coturn:latest
    container_name: healthcare-coturn
    ports:
      - "3478:3478/udp"
      - "3478:3478/tcp"
      # Small port range for Windows compatibility (avoids permission issues)
      # Using a small range that's less likely to conflict with Windows reserved ports
      - "49160-49200:49160-49200/udp"
      - "49160-49200:49160-49200/tcp"
    environment:
      - TZ=UTC
    command:
      - -n
      - --log-file=stdout
      - --listening-ip=0.0.0.0
      - --listening-port=3478
      # Small port range for Windows compatibility (41 ports should be sufficient)
      - --min-port=49160
      - --max-port=49200
      - --fingerprint
      - --lt-cred-mech
      - --user=openvidu:openvidu
      - --realm=openvidu
      - --no-cli
      - --no-tls
      - --no-dtls
    networks:
      - healthcare-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "turnutils_stunclient", "-p", "3478", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Jitsi Meet Services (Fallback Provider - like Redis in cache pattern)
  # COMMENTED OUT BY DEFAULT: OpenVidu is the primary video provider
  # Uncomment only if you need Jitsi as a fallback video provider
  # jitsi-prosody:
  #   image: jitsi/prosody:latest
  #   container_name: healthcare-jitsi-prosody
  #   environment:
  #     - AUTH_TYPE=internal
  #     - ENABLE_AUTH=1
  #     - ENABLE_GUESTS=1
  #     - GUEST_DOMAIN=guest.localhost
  #     - XMPP_DOMAIN=localhost
  #     - XMPP_AUTH_DOMAIN=auth.localhost
  #     - XMPP_GUEST_DOMAIN=guest.localhost
  #     - XMPP_MUC_DOMAIN=muc.localhost
  #     - XMPP_INTERNAL_MUC_DOMAIN=internal-muc.localhost
  #     - XMPP_RECORDER_DOMAIN=recorder.localhost
  #     - JICOFO_COMPONENT_SECRET=dev-jicofo-secret-change-in-production
  #     - JICOFO_AUTH_USER=focus
  #     - JICOFO_AUTH_PASSWORD=dev-focus-password-change-in-production
  #     - JVB_AUTH_PASSWORD=dev-jvb-password-change-in-production
  #     - JIGASI_XMPP_PASSWORD=dev-jigasi-password-change-in-production
  #     - JIBRI_RECORDER_PASSWORD=dev-jibri-password-change-in-production
  #     - JIBRI_XMPP_PASSWORD=dev-jibri-xmpp-password-change-in-production
  #     - JWT_APP_ID=${JITSI_APP_ID:-healthcare-jitsi-app}
  #     - JWT_APP_SECRET=${JITSI_APP_SECRET:-dev-jitsi-secret-change-in-production}
  #     - JWT_ACCEPTED_ISSUERS=healthcare-jitsi-app
  #     - JWT_ACCEPTED_AUDIENCES=healthcare-jitsi-app
  #     - ENABLE_JWT=1
  #     - PUBLIC_URL=${JITSI_BASE_URL:-https://localhost:8443}
  #     - TZ=UTC
  #   volumes:
  #     - jitsi_prosody_config:/config
  #     - jitsi_prosody_plugins:/prosody-plugins-custom
  #   networks:
  #     - healthcare-network
  #   restart: unless-stopped

  # jitsi-web:
  #   image: jitsi/web:latest
  #   container_name: healthcare-jitsi-web
  #   ports:
  #     - "8443:443"
  #     - "8080:80"
  #   environment:
  #     - ENABLE_AUTH=1
  #     - ENABLE_GUESTS=1
  #     - ENABLE_LETSENCRYPT=0
  #     - ENABLE_HTTP_REDIRECT=1
  #     - DISABLE_HTTPS=0
  #     - JICOFO_AUTH_USER=focus
  #     - XMPP_DOMAIN=localhost
  #     - XMPP_AUTH_DOMAIN=auth.localhost
  #     - XMPP_BOSH_URL_BASE=http://localhost:5280
  #     - XMPP_GUEST_DOMAIN=guest.localhost
  #     - XMPP_MUC_DOMAIN=muc.localhost
  #     - XMPP_RECORDER_DOMAIN=recorder.localhost
  #     - ENABLE_PREJOIN_PAGE=1
  #     - ENABLE_WELCOME_PAGE=1
  #     - ENABLE_CLOSE_PAGE=1
  #     - ENABLE_RECORDING=1
  #     - ENABLE_WAITING_ROOM=1
  #     - ENABLE_BREAKOUT_ROOMS=1
  #     - ENABLE_LOBBY=1
  #     - ENABLE_NOISY_MIC_DETECTION=1
  #     - ENABLE_TRANSCRIPTIONS=0
  #     - ENABLE_REMB=1
  #     - ENABLE_TCC=1
  #     - RESOLUTION=720
  #     - FRAME_RATE=30
  #     - CHANNEL_LAST_N=20
  #     - START_AUDIO_MUTED=10
  #     - START_VIDEO_MUTED=10
  #     - START_AUDIO_ONLY=0
  #     - DISABLE_DOMAIN_VERIFICATION=1
  #     - PUBLIC_URL=${JITSI_BASE_URL:-https://localhost:8443}
  #     - TZ=UTC
  #     - JWT_APP_ID=${JITSI_APP_ID:-healthcare-jitsi-app}
  #     - JWT_APP_SECRET=${JITSI_APP_SECRET:-dev-jitsi-secret-change-in-production}
  #     - JWT_ACCEPTED_ISSUERS=healthcare-jitsi-app
  #     - JWT_ACCEPTED_AUDIENCES=healthcare-jitsi-app
  #     - ENABLE_JWT=1
  #     - JWT_ASAP_KEYSERVER=${JITSI_BASE_URL:-https://localhost:8443}/asap
  #     - JWT_ALLOW_EMPTY=0
  #     - JWT_AUTH_TYPE=token
  #     - JWT_TOKEN_AUTH_MODULE=token_verification
  #   volumes:
  #     - jitsi_web_config:/config
  #     - jitsi_web_letsencrypt:/etc/letsencrypt
  #   depends_on:
  #     - jitsi-prosody
  #   networks:
  #     - healthcare-network
  #   restart: unless-stopped

  # jitsi-jicofo:
  #   image: jitsi/jicofo:latest
  #   container_name: healthcare-jitsi-jicofo
  #   environment:
  #     - AUTH_TYPE=internal
  #     - ENABLE_AUTH=1
  #     - XMPP_DOMAIN=localhost
  #     - XMPP_AUTH_DOMAIN=auth.localhost
  #     - XMPP_INTERNAL_MUC_DOMAIN=internal-muc.localhost
  #     - XMPP_SERVER=jitsi-prosody
  #     - JICOFO_COMPONENT_SECRET=dev-jicofo-secret-change-in-production
  #     - JICOFO_AUTH_USER=focus
  #     - JICOFO_AUTH_PASSWORD=dev-focus-password-change-in-production
  #     - JICOFO_RESERVATION_REST_BASE_URL=
  #     - JVB_BREWERY_MUC=jvbbrewery
  #     - JIGASI_BREWERY_MUC=jigasibrewery
  #     - JIGASI_SIP_URI=
  #     - JIBRI_BREWERY_MUC=jibribrewery
  #     - JIBRI_PENDING_TIMEOUT=90
  #     - TZ=UTC
  #   volumes:
  #     - jitsi_jicofo_config:/config
  #   depends_on:
  #     - jitsi-prosody
  #   networks:
  #     - healthcare-network
  #   restart: unless-stopped

  # jitsi-jvb:
  #   image: jitsi/jvb:latest
  #   container_name: healthcare-jitsi-jvb
  #   ports:
  #     - "10000:10000/udp"
  #     - "4443:4443"
  #   environment:
  #     - DOCKER_HOST_ADDRESS=127.0.0.1
  #     - XMPP_AUTH_DOMAIN=auth.localhost
  #     - XMPP_INTERNAL_MUC_DOMAIN=internal-muc.localhost
  #     - XMPP_SERVER=jitsi-prosody
  #     - JVB_AUTH_USER=jvb
  #     - JVB_AUTH_PASSWORD=dev-jvb-password-change-in-production
  #     - JVB_BREWERY_MUC=jvbbrewery
  #     - JVB_PORT=10000
  #     - JVB_TCP_HARVESTER_DISABLED=true
  #     - JVB_TCP_PORT=4443
  #     - JVB_STUN_SERVERS=stun.l.google.com:19302,stun1.l.google.com:19302
  #     - JVB_ENABLE_APIS=rest,colibri
  #     - JVB_WS_DOMAIN=localhost
  #     - JVB_WS_SERVER_ID=
  #     - TZ=UTC
  #   volumes:
  #     - jitsi_jvb_config:/config
  #   depends_on:
  #     - jitsi-prosody
  #     - jitsi-jicofo
  #   networks:
  #     - healthcare-network
  #   restart: unless-stopped
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: "2.0"
  #         memory: 2G
  #       reservations:
  #         cpus: "0.5"
  #         memory: 512M

volumes:
  healthcare_node_modules:
    name: healthcare_node_modules
  healthcare_pnpm_store:
    name: healthcare_pnpm_store
  postgres_data:
    name: healthcare_postgres_data
  dragonfly_data:
    name: healthcare_dragonfly_data
  redis_data:
    name: healthcare_redis_data
  pgadmin_data:
    name: healthcare_pgadmin_data
  openvidu_recordings:
    name: healthcare_openvidu_recordings
  openvidu_custom_layouts:
    name: healthcare_openvidu_custom_layouts
  jitsi_prosody_config:
    name: healthcare_jitsi_prosody_config
  jitsi_prosody_plugins:
    name: healthcare_jitsi_prosody_plugins
  jitsi_web_config:
    name: healthcare_jitsi_web_config
  jitsi_web_letsencrypt:
    name: healthcare_jitsi_web_letsencrypt
  jitsi_jicofo_config:
    name: healthcare_jitsi_jicofo_config
  jitsi_jvb_config:
    name: healthcare_jitsi_jvb_config

networks:
  healthcare-network:
    name: healthcare_network
    driver: bridge

