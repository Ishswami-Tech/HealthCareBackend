services:
  # Infrastructure Services (Profile: infrastructure)
  postgres:
    profiles: ["infrastructure"]
    image: postgres:16
    container_name: postgres
    hostname: postgres
    # SECURITY: Port 5432 removed from public exposure
    # PostgreSQL is only accessible from Docker network (172.18.0.0/16)
    # This prevents external brute force attacks
    # ports:
    #   - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data/pgdata
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: userdb
      # Optimized for 8 vCPU/24GB RAM: API (60) + Worker (30) + overhead = 120
      POSTGRES_MAX_CONNECTIONS: 120
      POSTGRES_SHARED_BUFFERS: 2GB
      PGDATA: /var/lib/postgresql/data/pgdata
    networks:
      app-network:
        ipv4_address: 172.18.0.2
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d userdb"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        # Optimized for 8 vCPU/24GB RAM: 700-900 concurrent users
        # Can run on 6 vCPU/12GB RAM initially (Docker enforces limits)
        limits:
          cpus: "3.0"      # Target: 8 vCPU server (3 CPU for PostgreSQL)
          memory: 10G      # Target: 24GB server (10GB for PostgreSQL)
        reservations:
          cpus: "1.5"      # Minimum guaranteed (works on 6 vCPU server)
          memory: 3G       # Minimum guaranteed (works on 12GB server)
    restart: unless-stopped
    shm_size: 256mb  # Shared memory for PostgreSQL
    stop_grace_period: 1m
    command: >
      postgres 
      -c max_connections=120 
      -c shared_buffers=2GB 
      -c effective_cache_size=12GB 
      -c maintenance_work_mem=256MB 
      -c checkpoint_completion_target=0.9 
      -c wal_buffers=32MB 
      -c default_statistics_target=100 
      -c random_page_cost=1.1 
      -c effective_io_concurrency=200 
      -c work_mem=16MB 
      -c min_wal_size=2GB 
      -c max_wal_size=8GB 
      -c max_worker_processes=8 
      -c max_parallel_workers_per_gather=4 
      -c max_parallel_workers=8 
      -c max_parallel_maintenance_workers=4 
      -c idle_in_transaction_session_timeout=60000 
      -c statement_timeout=60000 
      -c lock_timeout=60000 
      -c listen_addresses='*' 
      -c log_connections=on 
      -c log_disconnections=on 
      -c tcp_keepalives_idle=60 
      -c tcp_keepalives_interval=10 
      -c tcp_keepalives_count=3 
      -c client_min_messages=warning
    labels:
      - "com.docker.compose.service=postgres"
      - "app.component=database"
      - "app.type=postgres"
      - "app.healthcheck=enabled"
      - "app.persist=true"

  dragonfly:
    profiles: ["infrastructure"]
    image: docker.dragonflydb.io/dragonflydb/dragonfly:latest
    container_name: dragonfly
    hostname: dragonfly
    command: >
      --alsologtostderr
      --cache_mode=false
      --maxmemory=4gb
      --proactor_threads=6
      --logtostderr
      --default_lua_flags=allow-undeclared-keys
    ports:
      - "6380:6379"
    volumes:
      - dragonfly_data:/data
    networks:
      app-network:
        ipv4_address: 172.18.0.4
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -p 6379 ping || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        # Optimized for 8 vCPU/24GB RAM: 700-900 concurrent users
        # Can run on 6 vCPU/12GB RAM initially (Docker enforces limits)
        limits:
          cpus: "1.5"      # Target: 8 vCPU server (1.5 CPU for Dragonfly)
          memory: 4G       # Target: 24GB server (4GB for Dragonfly cache)
        reservations:
          cpus: "0.5"      # Minimum guaranteed (works on 6 vCPU server)
          memory: 1G       # Minimum guaranteed (works on 12GB server)
    restart: unless-stopped
    sysctls:
      net.core.somaxconn: 2048  # Increased from 1024 for more connections
    ulimits:
      nproc: 65535
      nofile:
        soft: 65535
        hard: 65535
    labels:
      - "com.docker.compose.service=dragonfly"
      - "app.component=cache"
      - "app.type=dragonfly"
      - "app.healthcheck=enabled"

  openvidu-server:
    profiles: ["infrastructure"]
    image: openvidu/openvidu-server-kms:latest
    container_name: openvidu-server
    hostname: openvidu-server
    ports:
      - "4443:4443"
      - "50000-50050:50000-50050/udp"
      - "50000-50050:50000-50050/tcp"
    environment:
      - OPENVIDU_SECRET=${OPENVIDU_SECRET:-MY_SECRET}
      - COTURN_SHARED_SECRET_KEY=${OPENVIDU_SECRET:-MY_SECRET}
      - OPENVIDU_PUBLICURL=${OPENVIDU_URL:-https://backend-service-v1-video.ishswami.in}
      - OPENVIDU_DOMAIN=${OPENVIDU_DOMAIN:-backend-service-v1-video.ishswami.in}
      - OPENVIDU_EDITION=ce
      - OPENVIDU_CDR=false
      - SERVER_PORT=4443
      - SERVER_SSL_ENABLED=false
      - HTTPS_PORT=4443
      - HTTP_PORT=5443
      - server.port=4443
      - server.address=0.0.0.0
      - OPENVIDU_WEBHOOK=${OPENVIDU_WEBHOOK_ENABLED:-false}
      - OPENVIDU_WEBHOOK_ENDPOINT=${OPENVIDU_WEBHOOK_ENDPOINT:-http://api:8088/api/v1/webhooks/openvidu}
      - OPENVIDU_WEBHOOK_EVENTS=["sessionCreated","sessionDestroyed","participantJoined","participantLeft"]
      - OPENVIDU_STREAMS_VIDEO_MAX_RECV_BANDWIDTH=1000
      - OPENVIDU_STREAMS_VIDEO_MIN_RECV_BANDWIDTH=300
      - OPENVIDU_STREAMS_VIDEO_MAX_SEND_BANDWIDTH=1000
      - OPENVIDU_STREAMS_VIDEO_MIN_SEND_BANDWIDTH=300
      - OPENVIDU_STREAMS_FORCED_VIDEO_CODEC=VP8
      - OPENVIDU_SESSIONS_GARBAGE_INTERVAL=900
      - OPENVIDU_SESSIONS_GARBAGE_THRESHOLD=3600
      - OPENVIDU_RECORDING=false
      - COTURN_IP=172.18.0.8
      - OPENVIDU_LOGS_LEVEL=WARN
      - DOMAIN_OR_PUBLIC_IP=${OPENVIDU_DOMAIN:-backend-service-v1-video.ishswami.in}
      - TZ=UTC
    volumes:
      - openvidu_recordings:/opt/openvidu/recordings
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      coturn:
        condition: service_healthy
    networks:
      app-network:
        ipv4_address: 172.18.0.7
        aliases:
          - openvidu-server
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "supervisorctl status | grep -q RUNNING || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "1.0"
          memory: 2G

  coturn:
    profiles: ["infrastructure"]
    image: coturn/coturn:latest
    container_name: coturn
    hostname: coturn
    ports:
      - "3478:3478/udp"
      - "3478:3478/tcp"
      - "5349:5349/udp"
      - "5349:5349/tcp"
      - "49160-49200:49160-49200/udp"
      - "49160-49200:49160-49200/tcp"
    environment:
      - TZ=UTC
      # Security: Use environment variable for password (set in .env.production)
      - COTURN_PASSWORD=${COTURN_PASSWORD:-openvidu}
      # Domain configuration: Use same domain as Video service (set in .env.production)
      - COTURN_DOMAIN=${COTURN_DOMAIN:-backend-service-v1-video.ishswami.in}
    volumes:
      - /etc/letsencrypt:/etc/letsencrypt:ro
    command:
      - -n
      - --log-file=stdout
      - --listening-ip=0.0.0.0
      - --listening-port=3478
      - --tls-listening-port=5349
      - --min-port=49160
      - --max-port=49200
      - --fingerprint
      - --lt-cred-mech
      # Security: Use environment variable for password instead of hardcoded
      - --user=openvidu:${COTURN_PASSWORD}
      # Use same domain as Video service
      - --realm=${COTURN_DOMAIN:-backend-service-v1-video.ishswami.in}
      - --server-name=${COTURN_DOMAIN:-backend-service-v1-video.ishswami.in}
      - --no-cli
      # TLS Configuration for TURNS
      # Certificates are mounted from host /etc/letsencrypt
      # If certificates don't exist at startup, coturn will log warnings but continue
      - --cert=/etc/letsencrypt/live/${COTURN_DOMAIN:-backend-service-v1-video.ishswami.in}/fullchain.pem
      - --pkey=/etc/letsencrypt/live/${COTURN_DOMAIN:-backend-service-v1-video.ishswami.in}/privkey.pem
      # Security: Resource limits to prevent exhaustion attacks
      - --max-bps=1000000
      - --max-sessions-per-user=10
      - --total-quota=1000000
      - --user-quota=1000000
      - --no-multicast-peers
      - --verbose
    networks:
      app-network:
        ipv4_address: 172.18.0.8
        aliases:
          - coturn
    restart: unless-stopped
    healthcheck:
      # More resilient health check: check if turnserver process is running
      # turnutils_stunclient may not be available in all coturn images
      test: ["CMD-SHELL", "pgrep -x turnserver > /dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # Security: Resource limits to prevent resource exhaustion attacks
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 512M

  # Application Services (Profile: app)
  api:
    profiles: ["app"]
    # Use image from GitHub Container Registry (for CI/CD deployments)
    # DOCKER_IMAGE is set by deploy.sh script
    # Prisma Client is generated during Docker build in CI/CD (see Dockerfile line 34)
    # NOT generated in CI lint job or at container startup - only during build
    image: ${DOCKER_IMAGE:-ghcr.io/ishswami-tech/healthcarebackend/healthcare-api:latest}
    # Local build section (commented out - we use pre-built images from registry)
    # build:
    #   context: ../..
    #   dockerfile: devops/docker/Dockerfile
    #   target: production
    #   pull: false
    container_name: latest-api
    hostname: api
    ports:
      - "${PORT:-8088}:${PORT:-8088}"
    env_file:
      - ../../.env.production
    volumes:
      - ./logs:/app/logs
      - /etc/letsencrypt:/etc/letsencrypt:ro
    environment:
      NODE_ENV: production
      DEV_MODE: "false"
      DOCKER_ENV: "true"
      # Disable HTTP/2 for testing compatibility (enable in real production with proper HTTP/2 clients)
      ENABLE_HTTP2: "false"
      # Use environment variables from .env.production (set via GitHub Actions)
      # ARCHITECTURE: Single Backend API, Multiple Frontends
      # - ONE backend API serves ALL clinics
      # - MULTIPLE frontends (one per clinic) connect to the SAME API
      # - Each frontend sends X-Clinic-ID header to identify which clinic it represents
      # - CORS_ORIGIN must include ALL clinic frontend URLs (comma-separated)
      CORS_ORIGIN: ${CORS_ORIGIN}
      FRONTEND_URL: ${FRONTEND_URL}
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}
      # Optimized for 8 vCPU/24GB RAM: 700-900 concurrent users, ~1200-1800 req/s (104M-156M requests/day)
      # Can run on 6 vCPU/12GB RAM initially (Docker will enforce limits)
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/userdb?connection_limit=60&pool_timeout=60&statement_timeout=60000&idle_in_transaction_session_timeout=60000&connect_timeout=60&pool_size=30&max_connections=60
      # Cache Provider Configuration (Dragonfly is the only cache provider in production)
      CACHE_PROVIDER: dragonfly
      
      # Dragonfly Configuration (primary cache provider)
      DRAGONFLY_ENABLED: "true"
      DRAGONFLY_HOST: dragonfly
      DRAGONFLY_PORT: 6379
      DRAGONFLY_KEY_PREFIX: "healthcare:"
      
      TRUST_PROXY: 1
      PRISMA_SCHEMA_PATH: ${PRISMA_SCHEMA_PATH:-/app/src/libs/infrastructure/database/prisma/schema.prisma}
      HOST: ${HOST:-0.0.0.0}
      PORT: ${PORT:-8088}
      BIND_ADDRESS: ${BIND_ADDRESS:-0.0.0.0}
      SOCKET_HOST: ${SOCKET_HOST:-0.0.0.0}
      SOCKET_PORT: ${SOCKET_PORT:-8088}
      NPM_CONFIG_LOGLEVEL: error
      # Use environment variables from .env.production (set via GitHub Actions)
      API_URL: ${API_URL}
      SWAGGER_URL: ${SWAGGER_URL:-/docs}
      BULL_BOARD_URL: ${BULL_BOARD_URL:-/queue-dashboard}
      SOCKET_URL: ${SOCKET_URL:-/socket.io}
      LOGGER_URL: ${LOGGER_URL:-/logger}
      BASE_URL: ${BASE_URL}
      API_PREFIX: ${API_PREFIX:-/api/v1}
      MAIN_DOMAIN: ${MAIN_DOMAIN:-viddhakarma.com}
      API_DOMAIN: ${API_DOMAIN:-backend-service-v1.ishswami.in}
      FRONTEND_DOMAIN: ${FRONTEND_DOMAIN:-viddhakarma.com}
      JWT_SECRET: ${JWT_SECRET:-your-super-secret-key-change-in-production}
      JWT_EXPIRATION: 24h
      
      # Session Configuration (Fastify Session with CacheService/Dragonfly)
      SESSION_SECRET: ${SESSION_SECRET:-your-session-secret-change-in-production-min-32-chars-long}
      SESSION_TIMEOUT: ${SESSION_TIMEOUT:-86400}
      SESSION_SECURE_COOKIES: ${SESSION_SECURE_COOKIES:-true}
      SESSION_SAME_SITE: ${SESSION_SAME_SITE:-strict}
      COOKIE_SECRET: ${COOKIE_SECRET:-your-cookie-secret-change-in-production-min-32-chars}
      
      LOG_LEVEL: info
      ENABLE_AUDIT_LOGS: "true"
      SECURITY_RATE_LIMIT: "true"
      # Optimized for 8 vCPU/24GB RAM: ~1200-1800 req/s peak capacity (700-900 concurrent users)
      SECURITY_RATE_LIMIT_MAX: 4000
      SECURITY_RATE_LIMIT_WINDOW_MS: 1000
      # Cache TTL and prefix (used by Dragonfly)
      CACHE_TTL: 3600
      CACHE_PREFIX: "healthcare:"
      # Rate limiting: Allow 600 req/min per IP (supports 800 users * 5 req/day + bursts)
      RATE_LIMIT_TTL: 60
      RATE_LIMIT_MAX: 600
      API_RATE_LIMIT: 1000
      AUTH_RATE_LIMIT: 30
      HEAVY_RATE_LIMIT: 50
      USER_RATE_LIMIT: 500
      HEALTH_RATE_LIMIT: 2000
      # Video Configuration
      VIDEO_ENABLED: ${VIDEO_ENABLED:-true}
      VIDEO_PROVIDER: ${VIDEO_PROVIDER:-openvidu}
      OPENVIDU_ENABLED: ${VIDEO_ENABLED:-true}
      OPENVIDU_URL: http://openvidu-server:4443
      OPENVIDU_SECRET: ${OPENVIDU_SECRET:-MY_SECRET}
    expose:
      - "${PORT:-8088}"
    networks:
      app-network:
        ipv4_address: 172.18.0.5
        aliases:
          - backend-service-v1.ishswami.in
    dns: []
    extra_hosts:
      - "api:172.18.0.5"
      - "postgres:172.18.0.2"
      - "dragonfly:172.18.0.4"
      - "openvidu-server:172.18.0.7"
      - "coturn:172.18.0.8"
    depends_on:
      postgres:
        condition: service_healthy
      dragonfly:
        condition: service_healthy
      coturn:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:${PORT:-8088}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        # Optimized for 8 vCPU/24GB RAM: 700-900 concurrent users, ~1200-1800 req/s
        # Can run on 6 vCPU/12GB RAM initially (Docker enforces limits)
        limits:
          cpus: "3.0"      # Target: 8 vCPU server (3 CPU for API)
          memory: 6G       # Target: 24GB server (6GB for API)
        reservations:
          cpus: "1.5"      # Minimum guaranteed (works on 6 vCPU server)
          memory: 2G       # Minimum guaranteed (works on 12GB server)
    # Migrations run automatically via CMD in Dockerfile before starting the app
    # No need to override command - Dockerfile CMD handles migrations + app start
    # Prisma Client is generated during Docker build, entrypoint verifies it exists
    labels:
      - "com.docker.compose.service=api"
      - "app.component=backend"
      - "app.type=api"

  worker:
    profiles: ["app"]
    # Use same image as API (worker runs from same codebase)
    # DOCKER_IMAGE is set by deploy.sh script
    # Prisma Client is generated during Docker build in CI/CD (see Dockerfile line 34)
    # NOT generated in CI lint job or at container startup - only during build
    image: ${DOCKER_IMAGE:-ghcr.io/ishswami-tech/healthcarebackend/healthcare-api:latest}
    # Local build section (commented out - we use pre-built images from registry)
    # build:
    #   context: ../..
    #   dockerfile: devops/docker/Dockerfile
    #   target: production
    container_name: latest-worker
    hostname: worker
    env_file:
      - ../../.env.production
    environment:
      NODE_ENV: production
      APP_MODE: worker
      SERVICE_NAME: worker
      DEV_MODE: "false"
      DOCKER_ENV: "true"
      
      # Database Configuration - Optimized for 8 vCPU/24GB RAM: 700-900 concurrent users
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/userdb?connection_limit=30&pool_timeout=60&statement_timeout=60000&idle_in_transaction_session_timeout=60000&connect_timeout=60&pool_size=15&max_connections=30
      PRISMA_SCHEMA_PATH: /app/src/libs/infrastructure/database/prisma/schema.prisma
      
      # Cache Provider Configuration (Dragonfly is the only cache provider in production)
      CACHE_PROVIDER: dragonfly
      CACHE_ENABLED: "true"
      DRAGONFLY_ENABLED: "true"
      DRAGONFLY_HOST: dragonfly
      DRAGONFLY_PORT: 6379
      DRAGONFLY_KEY_PREFIX: "healthcare:"
      
      # JWT Configuration (for queue operations)
      JWT_SECRET: ${JWT_SECRET:-your-super-secret-key-change-in-production}
      
      # BullMQ Worker Configuration - Optimized for 8 vCPU/24GB RAM: 700-900 concurrent users
      BULL_WORKER_CONCURRENCY: "10"
      BULL_MAX_JOBS_PER_WORKER: "100"
      
      # Logging Configuration
      LOG_LEVEL: info
      ENABLE_AUDIT_LOGS: "true"
      
      # Email Configuration (for queue jobs)
      AWS_REGION: ${AWS_REGION:-}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-}
      
      # Firebase Configuration (for notifications)
      FIREBASE_PROJECT_ID: ${FIREBASE_PROJECT_ID:-}
      FIREBASE_PRIVATE_KEY: ${FIREBASE_PRIVATE_KEY:-}
      FIREBASE_CLIENT_EMAIL: ${FIREBASE_CLIENT_EMAIL:-}
    volumes:
      - ./logs:/app/logs
    networks:
      app-network:
        ipv4_address: 172.18.0.6
        aliases:
          - worker.ishswami.in
    dns: []
    extra_hosts:
      - "worker:172.18.0.6"
      - "postgres:172.18.0.2"
      - "dragonfly:172.18.0.4"
      - "openvidu-server:172.18.0.7"
      - "coturn:172.18.0.8"
    depends_on:
      postgres:
        condition: service_healthy
      dragonfly:
        condition: service_healthy
      api:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "node", "-e", "process.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        # Optimized for 8 vCPU/24GB RAM: 700-900 concurrent users
        # Can run on 6 vCPU/12GB RAM initially (Docker enforces limits)
        limits:
          cpus: "1.0"      # Target: 8 vCPU server (1 CPU for Worker)
          memory: 2G        # Target: 24GB server (2GB for Worker)
        reservations:
          cpus: "0.5"       # Minimum guaranteed (works on 6 vCPU server)
          memory: 512M      # Minimum guaranteed (works on 12GB server)
    # Worker doesn't need migrations - only API container runs them
    command: ["node", "dist/worker-bootstrap.js"]
    labels:
      - "com.docker.compose.service=worker"
      - "app.component=worker"
      - "app.type=worker"

  # Portainer - Docker Management UI (like Vercel Dashboard)
  # Provides comprehensive Docker management: containers, images, networks, volumes, logs, stats
  # Access at: http://localhost:9000 (or https://your-domain:9000 in production)
  # Optimized for minimal disk usage: limited logs, no snapshots, strict resource limits
  portainer:
    profiles: ["infrastructure"]
    image: portainer/portainer-ce:latest
    container_name: portainer
    ports:
      - "9000:9000"
      - "9443:9443"  # HTTPS port (optional)
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    networks:
      app-network:
        ipv4_address: 172.18.0.9
    restart: unless-stopped
    # Optimized command: minimal logging, no analytics
    command: >
      -H unix:///var/run/docker.sock
      --no-analytics
    environment:
      # Minimize disk usage: reduce logging, disable telemetry
      - PORTAINER_LOG_LEVEL=WARN  # Only WARN and ERROR logs (reduces log file size)
      - PORTAINER_TELEMETRY=false  # Disable telemetry data collection
    # Strict resource limits to prevent excessive disk usage
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M  # Minimal memory allocation
        reservations:
          cpus: "0.1"
          memory: 128M
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/api/system/status || exit 1"]
      interval: 30s  # Reduced frequency to save resources
      timeout: 5s
      retries: 3
      start_period: 30s

volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/healthcare-backend/data/postgres
  dragonfly_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/healthcare-backend/data/dragonfly
  openvidu_recordings:
    name: latest_openvidu_recordings
  portainer_data:
    name: latest_portainer_data
    # Portainer data is minimal: only stores user preferences and settings
    # Expected size: < 50MB (snapshots disabled, minimal logging)

networks:
  app-network:
    name: app-network
    ipam:
      config:
        - subnet: 172.18.0.0/16

